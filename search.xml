<?xml version="1.0" encoding="utf-8"?>
<search>
  
    
    <entry>
      <title><![CDATA[《神经网络与深度学习》学习笔记]]></title>
      <url>%2F2017%2F05%2F07%2F%E3%80%8A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
      <content type="text"><![CDATA[康肃问曰：”汝亦知射乎？吾射不亦精乎？”翁曰：”无他， 但手熟尔。” 目录 神经网络进行学习的基本前提 神经网络两个假设与四个等式 实现 超参数的选择 存在的问题与优化点 为什么神经网络能够『解决』『所有』问题 神经网络进行学习的基本前提神经网络对输入样本点的学习，主要是根据不断修改权值与偏倚值，保证输出结果朝着可预期的方向不断改善。为了了解整个过程，我们首先观察权值矩阵和偏倚值矩阵的变化如何影响最终结果的输出。当我们对权值矩阵进行了一个微小的修改： 这个修改首先会反应在该神经元的激活值上：这个激活值的变化会影响所有下一层邻接神经元的激活值，并最终反应到输出层的变化：如果选中其中一条变化的路线，可以用公式描述整个过程：如果将中间多个隐藏层与不同路线的变化当做一个黑盒，可以用公式来描述整个过程：其中，描述的是最终结果对wjk的敏感程度，该公式也可以根据链式法则+穷举所有路径，展开获得完整的变化。 上述变化过程，在整个神经网络训练过程中不断重复，通过对权值矩阵和偏倚值矩阵不断『轻微』的『试探』，刺激输出值进行一个『轻微』的反馈，根据反馈的优劣改变刺激方向与程度，寻找一个相对可行的优化『路线』，并最终到达一个『最优值』。重复这个过程，就能保证整个网络在数据的基础上正常的运作起来。如下图所示：因此，问题的关键在于保证『轻微』的变化会给予输出结果『轻微』的变化，如果可以，我们希望这种变化『朴素』到仅仅与变化源头的神经元存在关联。如上述公式所描述，最终结果的变化为权值与偏倚值变化的线性变化，这样，我们就可以控制Delta值的系数，以此保证了输出值变化的可控程度。 在此基础上，我们考虑，如何来衡量『轻微』。『轻微』的含义是，变化源头的神经源微小的变化，不论中间隐藏层各个神经元如何变化，对输出也造成相同程度的变化。因此我们需要保证激活值的变化是相对『连续』的。如果选择阶跃函数作为激活函数，这一个输入的轻微变化可能导致输出由0变成1，进而影响下一层所有邻接神经元，最终对输出造成极大的变化。因而，在神经网络中，经常选择sigmoid function作为激活函数，这类神经元也被称为S型神经元。权值与偏倚值值的微小变化，同样反应在输出值的微小变化。 神经网络两个假设与四个等式代价函数的两个假设最简单的代价函数是用来衡量输出结果与真实结果间的一种误差程度，进而用来知道权值矩阵和偏倚值矩阵的优化方向。 最小二乘代价函数：交叉熵代价函数： 关于代价函数的选择，有两个基本的假设作为前提。 假设1代价函数能够被写成上述表达形式，其中C为整体代价，Cx为单个训练样本点的代价。该假设存在的原因是在训练过程中，都是以单个样本点作为单位进行训练。 假设2第二条假设是代价函数能够被写成最终激活值的函数表示，如下所示：该假设存在的原因是只有激活值是直接与权值矩阵，偏倚值矩阵关联在一起，因此只有将代价函数与激活值关联在一起，才能对这两者优化方向进行判定。 BP背后的四个基本等式基于梯度下降算法的神经网络训练在上世纪八十年代就已经被提出，但是，由于weight与biases的梯度计算，需要考虑所有可能的路线，每条路线需要根据链式法则计算各个偏导值，这些偏导值存在大量的重复计算，当隐藏层数量与神经元数量增加，计算量将爆炸增长。可行的解决方案包括使用DP记录各个偏导的计算结果，以空间换时间的方式来降低计算时间，不过不知道为什么DP并没有应用在神经网络的训练中。 后来，有大神提出了通过后向反馈将误差值逐层『平摊』到各个神经元上，而平摊考虑到某个神经元对下一层邻接神经元的『贡献量』，而『贡献量』关联了权值矩阵和偏倚值矩阵的梯度，这个梯度又被应用与梯度下降时目标的学习速率，整个过程只需要从后向前传播一次即可。这种方法极大降低了神经网络训练过程中的计算量，也标志这神经网络训练的实用化。 提到学习速率这个指标，有些资料会说η就是学习速率。 η真正代表的是学习步长。在梯度下降训练中，η（步长）表示每次沿梯度最大方向下降一步的距离。真正的学习速率，应该是权值矩阵与偏倚值矩阵变化的速度，可以简单认为是梯度的大小，梯度越大，代表当前的学习速率越快。 整个后向反馈神经网络的训练，就是将一个误差变量与各个偏导关联在一起，通过这个中间变量来完成偏导的获取，进而实现更新。这两者的关系，可以浓缩为四个基本等式： BP1方程一描述的是误差变量的定义，该变量是整体误差平摊到各个子神经元的『贡献』总量的一个度量，这个变量存在的目的是将反向传播过程中权值矩阵和偏倚值值矩阵的梯度值联系起来。 BP2方程二描述的是反向传播过程中误差值的传播，从输出层的误差向前传播，并将误差逐层记录。该等式的存在，表明了误差变量的递推关系，前一层可以根据后一层的误差计算出来，推导过程如下： BP3方程三将误差变量与偏倚值矩阵的梯度关联在一起 BP4方程四将误差变量与权重矩阵的梯度关联在一起 通过上述的四个等式，我们可以将梯度值的获取由链式法则的计算修改为误差传播的方式来获取。 实现两个假设和BP1-4指定了在神经网络训练过程中权值矩阵和偏倚值矩阵梯度的获取方式，通过类似DP的方式降低了求导的计算量，具体计算过程如下描述： 12345678910111213def SGD(self, train_data, epoll, mini_batch_size, eta, test_data = None): if test_data: n_test = len(test_data) n = len(train_data) for i in xrange(epoll): random.shuffle(train_data) mini_batches = [train_data[j:j+mini_batch_size] for j in xrange(0, n, mini_batch_size)] for mini_batch in mini_batches: self.update_mini_batch(mini_batch, eta) if test_data: print "Epoll &#123;0&#125;: &#123;1&#125; / &#123;2&#125;".format(i,self.evaluate(test_data),n_test) else: print "Epoll &#123;0&#125; complete".format(i) SGD是入口函数，主要完成训练样本的随机散列和分批计算，通过调用update_mini_batch，完成批数据的训练 1234567891011121314151617def backpop(self, x, y): nndl_w = [np.zeros(np.shape(item)) for item in self.weights] nndl_b = [np.zeros(np.shape(item)) for item in self.biases] z, zs, activations = self.feedforword(x) #delta = self.cost_derivative(activations[-1], y) * sigmoid_prime(z) delta = self.cost_derivative(activations[-1], y) nndl_b[-1] = delta nndl_w[-1] = np.dot(delta, activations[-2].transpose()) for l in xrange(2, self.num_layers): delta = np.dot(self.weights[-l+1].transpose(),delta) * sigmoid_prime(zs[-l]) nndl_b[-l] = delta nndl_w[-l] = np.dot(delta, activations[-l-1].transpose()) return nndl_b, nndl_w backpop是在反向传播过程中，逐层计算误差变量的值，并根据BP3与BP4更新权值矩阵和偏倚值矩阵的梯度。 1234567891011def update_mini_batch(self, mini_batch, eta): nndl_w = [np.zeros(item.shape) for item in self.weights] nndl_b = [np.zeros(item.shape) for item in self.biases] for x,y in mini_batch: b, w = self.backpop(x, y) nndl_b = [b + nb for b, nb in zip(nndl_b, b)] nndl_w = [w + nw for w, nw in zip(nndl_w, w)] self.biases= [b - (eta/len(mini_batch))*nb for b, nb in zip(self.biases, nndl_b)] self.weights = [w - (eta/len(mini_batch))*nw for w, nw in zip(self.weights, nndl_w)] update_mini_batch主要完成样本点的前向传播，后向反馈过程，并在后向反馈过程中更新权值矩阵和偏倚值矩阵。其中： backpop完成后向反馈过程，并将误差变量记录 weight和biases的更新使用梯度下降方式更新 超参数的选择超参数主要包括4种 批训练样本数量 训练迭代次数 步长 正则化比率（lamda） 参数的选择有很多启发式规则，总结如下 在训练初期，减小问题与样本规模、难度 快速试错，快速反馈 更频繁的监控结果，减少监控需要的计算数量 存在的问题与优化点网络训练过程中，最终要的有两点，训练时的学习速率和过拟合的抑制，前者保证网络能够更快的获取结果，特别是在训练初期，对超超速调整时；后者主要衡量训练出来的模型是否很好的抽象出一般性的通用规则。 学习速率的优化之前已经提到过，学习速率指的是权值矩阵和偏倚值矩阵朝着最优值方向更新的速度，这个速度代表这算法收敛到一个最优模型所花费的时间，因此这个速率是很重要的。 学习速率慢的真正原因在SGD中，初始权值矩阵和偏倚值矩阵的初始值是随机的，可以这么认为，在三维空间中，初始点的位置是随机的，我们将从一个随机的位置开始使用梯度下降进行最优值的搜寻。 当我们的初始值w=0.6,b=0.9时，学习速率的变化，特别是在初始阶段的变化比较迅速，在逐渐接近最优值后，速度开始放缓。 当我们的初始值w=2.0,b=2.0时，学习速率的变化在初始阶段特别缓慢，学习一段时间后，才开始加速整个学习过程。 考虑到我们在日常生活中面对的情景，如果面对一个 完全不熟悉的东西，特别是在误差特别明显的情况下，我们应该能够更快的调整并极大降低误差，这是我们对学习这件事的本能。但是在上述SGD的学习过程中，表现和我们的本能完全相反，那学习速率缓慢的真正原因是？下图是SGD中权值矩阵的更新方式其中最重要的一项是w的梯度其中有一项是激活函数的偏导，根据之前的描述，我们一直选择sigmoid函数作为激活函数，观察sigmoid的函数曲线 sigmoid函数在x&lt;&lt;0时，导数约等于0 sigmoid函数在x&gt;&gt;0时，导数约等于0 这告诉我们，如果激活函数选择为sigmoid function，那么，在误差很大和误差很小的情况下，导数都约等于0，进而造成w和b的梯度接近0，这就是最小二乘的代价函数+sigmoid函数组合时造成学习速率更新慢的根本原因。 交叉熵解决方案所以，面对学习速率慢的问题，解决方案就是取消梯度中sigmoid函数导数这一项，但是，像sigmoid或者tanh这样的激活函数，具有很多其他函数不具备的优点，因此修改激活函数并非最优的解决方案。 取代的解决方案是修改最小二乘来衡量代价函数，使用交叉熵代价函数关于交叉熵的推到过程，最根本的想法是取消梯度中的导数，然后使用积分方法推到出。至于交叉熵代表的含义，涉及到信息论中的内容，大概解释是，交叉熵衡量了一种混乱的程度，如果这种混乱程度能够降低到一个最小值，这个最小值代表了当前环境一种稳定的情况。但选择了交叉熵代价函数和sigmoid函数的组合后，w的梯度变为可以发现，激活函数的偏导项已经取消，同样对b的梯度公式也取消了导数。 关于代价函数的修改对算法的影响首先考虑，在BP1-4中，只有BP1直接与代价函数C存在关联，所以，整个算法中需要修改的仅仅是前向传播结束，后向反馈开始，整体误差的计算公式会发生变化。其他部分还是和原来一样 softmax解决方案略 权值矩阵的初始化在梯度下降中，初始位置使用随机值，这个随机值服从标准正态分布现在考虑一个简化的情景，假设我们已经用标准正态分布初始化了第一层的权重值假设我们有1000个输入节点，500个输出1，500个输出0，这种情况下z受到501个参数的影响，z服从均值为0，标准差为sqrt(501)的正态分布 这种情况和我们之前讨论过为什么最小二乘的代价函数配合sigmoid函数造成学习速度降低是一个道理。 如果我们对权值矩阵初始值做归一化操作，最终的z分布呈现这种情况，将尽可能减少学习速率降低的情况，在误差大的情况下，提高学习速率 详细描述 过拟合的抑制过拟合的问题是过度关注样本点在测量时的噪音，而忽略了真正隐藏在数据下面的通用性规律，因此需要对权值矩阵进行一定的限制。 L2正则化L2 的意思是平方，正则化也称为权重衰减，具体描述第一部分是交叉熵代价函数，第二部分是正则项，添加第二部分后对BP的影响 为什么神经网络能够『解决』『所有』问题神经网络能够解决什么样的问题？ 答案是，能解决所有的问题。 『解决』的含义神经网络中对于解决的问题的定义是，能够以近似值的方式，尽量『趋近』真实的解决方案。这个解决方案是未知的，也不知道应该如何获取这个解决方案，但是，网络能够提供一种『最优解』，这个最优解能够提供接近真实解决方案的一个解决方案。这就是神经网络解决的含义 『所有』的含义首先，对问题规模进行一个精简，假设我们需要计算下面这个奇怪复杂的函数在某个x值点上y的值面对这个函数曲线，我们既不知道这个函数背后的数学公式，也不知道问题的解决方案。但我们知道，不管函数什么样，总会有个输入x和输出y，我们不知道f(x)是什么，但是我们可以获得一个y’约等于f(x)。即时这个函数存在一个多维输入，通用可以将问题抽象为： 这意味着，神经网络具有一种普遍性，不论需要计算的是什么样的函数，我们都确定存在一个网络架构能够计算这个问题。关键点在于，我们并没有真正获取函数对应的数学公式，我们也不知道真实的解是多少，但是我们可以获得一个近似值，并且这个近似值对应的误差是可控的。同时，我们的网络架构具备普适性，能够面对任意的函数。如果将函数推广到需要解决的问题： f(x) 代表将中文翻译为对应的英文 f(x) 代表将混合的多源语音进行分离 f(x) 代表将人脸图片映射为代表的人 神经网络的这种普适性保证了我们能够解决『所有问题』。 可能存在的一个问题是，上述的普适性，面对的函数是连续的，如果函数是非连续的，类似阶跃函数这样的情景下，神经网络就不适应。 如何解决前两节只是描述了神经网络能够对连续函数的输出值进行一个估计，并将误差控制在一个阈值内。那网络是如何解决这个问题？ 我们先简化问题规模，解决的f(x)只有一维输入和一维输出，首先关注最上面的神经元。 增加w的值，对应的输出将逐渐由sigmoid函数变化为阶跃函数 改变b的值，将平移输出结果 这里之所以要用阶跃函数取代sigmoid函数来分析问题，主要是一系列阶跃函数的累加值分析更加直观简单，相反，一连串s型的曲线叠加在一起是什么就更难分析 关键点就在于阶跃函数发生阶跃的位置，这个位置可以由b控制，同时令s=-b/w，可以得到下述表现形式 我们控制w1的值，就可以控制阶跃的高度，两个阶跃叠加的结果 如果w1和w2的值相反，正负叠加的结果为一个单峰的阶跃值。 上述叠加发生在存在两个配对的神经元间，如果我们增加神经元的数量，则可以通过叠加获得更加丰富的结果。现在回到最早的问题，我们要解决一个特殊的函数这个函数实际上对应的函数是虽然我们并不知道，但通过多个阶跃值进行模拟，就可以获得一个粗略的模拟值和一个与真实值自建的差异通过控制各个s与h的值，就能够保证在可以接受的误差阈值内获取一个模型，这个模型能够尽量接近真实结果，这就是神经网络能够训练所有问题的基础。 后序 上述内容是在学习《神经网络与深度学习》之后的一些感悟，总结的有点杂乱，大概描述了看这本书过程中一些重要的理解。然后我决定以后看一本书，就写一篇读书笔记，花了一个月的时间，总不能就这样什么都没留下是吧。 看这本书，源自于某两人在某次开会时争论深度学习与机器学习是不是完全不同的两种技术。 以前还在糯米的时候，听晓大神说过，神经网络与深度学习就是把一堆输入提供给网络，我们不知道它是怎么训练的，也不知道各个中间值是什么意思，代表什么。这是深度学习和机器学习特别是监督学习最本质的区别。 看完nndl，要说能有什么马上能转化为产出的地方，其实并没有。但是，很多年前上编译原理的时候，大保健就说过，你花在读书上的时间永远不会浪费，哪怕这个东西你现在用不上，哪怕你过段时间都快忘了，总有一天，当你突然需要用时，泉思如涌，自然而然，厚积薄发。 看这本书的时候，我一直想起以前泡图书馆的日子，悠闲自在，有大把大把的时间可以去做自己想做的事。但是，那时候傻啊，屁都不懂就特么闷头学.NET，还看不懂英文书，白特么瞎了那么多时间。讲真，我适合搞学术。 然后，没然后了，上了年纪不比当年，反应有点慢。就这样吧，下本看的书是Bishop老爷子的在95年写的《Neural Networks for Pattern Recognition》，听闻比PRML简单。 提到PRML，我心里那个恨啊，特别是最近发现有个哈工大的大神，翻译了一整本书，92年的啊，92年的啊，我日了啊。 最后的最后，曾经有人看了文章，迫于我的气场，给我打赏了一分钱，没错，就特么一分钱，我祝他短1cm，张小哥，讲的就是你。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[《Node.js学习笔记（一）：内存控制与垃圾回收》]]></title>
      <url>%2F2016%2F12%2F01%2Fnode-gc-part1%2F</url>
      <content type="text"><![CDATA[Node就是一群前端的自嗨 &lt;-未完待续-&gt; V8的垃圾回收机制Scavenge算法之前已经提到过，在V8将内存分为新生代和老生代两部分。而内存的分配主要集中在新生代部分，同时，新生代部分因为存活对象比较少，因此，新生代部分的垃圾回收主要使用scavenge算法。算法将新生代分成From区和To区，内存的分配主要是在From区中进行。当内存分配过程中allocationPtr指针到达From区结尾，就将触发垃圾回收机制。 可达性分析：即通过一系列称为GC ROOT的对象作为起点，从这些节点开始向下进行搜索，搜索走过的路径称为引用链,当一个对象到GC ROOT没有任何引用链，则证明该对象不可达。 一个等价约定：如果一个对象可经由某个被定义为活跃对象的对象，通过某个指针链所访问，则它就是活跃的。其他的都被视为垃圾 算法基本流程算法对应的伪码如下描述：12345678910111213141516171819202122232425262728293031def scavenge(): swap(fromSpace, toSpace) allocationPtr = toSpace.bottom scanPtr = toSpace.bottom for i = 0..len(roots): root = roots[i] if inFromSpace(root): rootCopy = copyObject(&amp;allocationPtr, root) setForwardingAddress(root, rootCopy) roots[i] = rootCopy while scanPtr &lt; allocationPtr: obj = object at scanPtr scanPtr += size(obj) n = sizeInWords(obj) for i = 0..n: if isPointer(obj[i]) and not inOldSpace(obj[i]): fromNeighbor = obj[i] if hasForwardingAddress(fromNeighbor): toNeighbor = getForwardingAddress(fromNeighbor) else: toNeighbor = copyObject(&amp;allocationPtr, fromNeighbor) setForwardingAddress(fromNeighbor, toNeighbor) obj[i] = toNeighbordef copyObject(*allocationPtr, object): copy = *allocationPtr *allocationPtr += size(object) memcpy(copy, object, size(object)) return copy 算法主要有下述几个步骤组成： 基本环境值修改，如交换From/To区指针等信息 将GC ROOT根节点所有子节点从From区移动到To区 复制剩余存活节点 第二步复制结束后，allocationPtr指向下一个可复制位置，scanPtr指向第一个开始复制的位置 对[scanPtr,allocationPtr)这段区间中的每个对象进行判断 如果是指针，并且不在老生代中 如果已经移动到To区，直接修改指针指向的内存地址 否则，将指针指向的对象移动到To区，同时修改指针指向的内存地址 重复整个流程，指导scanPtr = allocationPtr 整个流程其实类似一个广度优先搜索，从GC ROOT开始，一层一层地往下搜索，直到所有被引用的存活对象都被访问并且被移动到To区为止。 写屏障上节讲到，在存活对象的遍历过程中，会根据GC ROOT判断某个对象是否存活。如果有个位于老生代的指针指向了新生代中某个对象，这时候如果需要去遍历整个老生代，在时间上的消耗将很大。V8的解决方案是在老生代指针指向新生代对象时设置一个写屏障。 当一个新生代对象刚生成的时候，并不存在一个指向它的指针 当存在一个老生代对象指向它时，在写屏障中记录这个信息 当一个新生代对象晋升时，在写屏障中记录这个信息 Mark-Sweep&amp;Mark-Compact算法​ Scavenge算法之所以在新生代垃圾回收中能够有效运行，很大程度上取决于新生代对象在一次垃圾回收后存活对象比较少。但是，老生代对象在每轮垃圾回收中都会有大量的存活对象，这时候如果使用Scavenge算法，将不可避免造成大量的内存移动，因此，V8中使用Mark-Sweep和Mark-Compact算法进行垃圾回收。 标记算法 位图区：内存页还含有一个页头（包含一些元数据和标识信息）以及一个位图区（用以标记哪些对象是活跃的），位示图是利用二进制的一位来表示磁盘中一个块的作用情况，当其值为0 时表示对应盘块空闲；值为1时盘块已分配。磁盘上所有盘块都有一个二进制位与之对应，这样，由所有盘块所对应的位形成了一个集合称为位示图，位示图用磁盘块存放，称为位图块。 在标记阶段，每个页都会有一个页图用来做标记的位图，位图中每一位代表页中一个字（注：一个字就是一个指针的大小，考虑到指针可能在一页中任何位子出现，） 标记清除和标记压缩算法的第一步都是标记存活对象，具体流程如下伪码所示： 1234567891011121314151617181920212223242526272829303132333435markingDeque = []overflow = falsedef markHeap(): for root in roots: mark(root) do: if overflow: overflow = false refillMarkingDeque() while !markingDeque.isEmpty(): obj = markingDeque.pop() setMarkBits(obj, BLACK) for neighbor in neighbors(obj): mark(neighbor) while overflow def mark(obj): if markBits(obj) == WHITE: setMarkBits(obj, GREY) if markingDeque.isFull(): overflow = true else: markingDeque.push(obj)def refillMarkingDeque(): for each obj on heap: if markBits(obj) == GREY: markingDeque.push(obj) if markingDeque.isFull(): overflow = true return ### 参考文献 https://bugs.chromium.org/p/v8/issues/detail?id=847 http://www.linuxidc.com/Linux/2015-03/115186.htm http://t.viewpoint.gr/node/deps/v8/src/heap.cc http://gold.xitu.io/entry/564ae48200b0d1db3385688e https://github.com/drewfish/node-tick-processor http://newhtml.net/v8-garbage-collection/ http://www.csdn.net/article/1970-01-01/2826316]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[序]]></title>
      <url>%2F2016%2F11%2F24%2Fhelloworld%2F</url>
      <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;世界上有两件东西能够深深地震撼人们的心灵，一件是我们心中崇高的道德准则，另一件是我们头顶上灿烂的星空。 过去&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;引言其实和这篇序没什么关系，只不过是我很喜欢的一句话。曾经有一个大神大晚上赤裸上半身、抱着把吉他来213寝室，深情的说了这句我很喜欢的话，虽然我忘了上下文，虽然我觉得他单纯为了装逼，不过，我喜欢。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;到现在我还记得很多年前那个夏天的早晨，九点的教学楼到处弥漫着那个小岛上夏天特有的味道，慵懒的阳光和通宵打游戏后朦胧的睁不开的双眼。不为什么，只是感觉，那几年是人生中最好的时光，懒散、无虑，有大把大把的时间可以瘫在图书馆顶楼的沙发上思考自己想做什么，虽然一般除了抄作业就是看小说。我其实很喜欢小岛上的时光，我想念烧仙草、大开杯、海蛎煎、黄则和还有大径村那条小吃街上所有的小吃，还有那个我吃了好几年没变过的香菇鸡丁盖浇饭配纯牛奶，完美。虽然养成了拖延癌晚期，但那里改变了我的一切。 现在&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;一眨眼来帝都快一年了，一年前的这时候，我还在先研院的单间里通宵打游戏，那时候我做梦都想不到，现在会在这个离家一千多里的地方一个人学习、工作、生活。记得那时候刚下火车，我好奇的看了眼地铁口茫茫人海，琢磨了好久才想明白自己已经在一个完全陌生的地方，一个自己几个月前还坚定的认为打死也不来的地方，去面对一群没见过面的人。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;人生就是这么的奇妙。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;有的时候，拼命想去获取一个看似应该属于自己的东西，一个晃神老天就狠狠散了你一巴掌，然后缺心眼地嘲讽你的无能。命是个很操蛋的东西，不是你的东西，你怎么努力抢也抢不到，关键你还会觉得这真的是你的东西啊。就像很久以前，我就是想找个离家近点的学校，最后脸被啪啪啪地抽肿。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我一直觉得自己是个不爱折腾不思进取不求变化的人，偏偏误打误撞当了个程序员，当然不思进取可能有点过。作为一名非典型码农，你要我开篇写个Helloworld，诶，我觉得你是在污辱我情操，但凡有点底线的码农都不会跟风写个Helloworld，你写个HelloKitty都行，虽然码农并不是什么很丢脸的事。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;其实我费尽心思琢磨了好久，想写点带点灰色嘲讽的『序』来凸显自己的高逼格，以作为哥哥职业生涯的开端，然后才发现，自己已经不比当年的才思泉涌，上了年纪的人，思绪有点慢，那就酱紫吧。 未来&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;多年前我第一次踏进大学校门的时候，就明白了一个道理。大家都说，没有人会知道自己的未来是怎样。其实这话不对。大家都知道自己的未来是什么样的，但是又不敢承认，也不知道这个未来要多久，有多难，这个未来和我预期的有多大的契合度，自己能不能在这个过程中坚持下来，又愿意付出多少。这事其实很残酷。然后，还有很多鸡汤会和你说，年轻人不要急，一步一步慢慢来。哔哔哔哔，如果年轻的时候，我一定会这么说，我不远千里跑到这人生地不熟的地方你叫我慢慢来，是你逗我还是想我逗你。不过想想现在都二十好几了，还是慢慢来吧。 后记&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;大晚上的，居然有个小哥说我浪，诶，人如果不浪，那和咸鱼有什么区别？]]></content>
    </entry>

    
  
  
</search>
