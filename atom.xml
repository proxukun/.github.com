<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>proxukun</title>
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2017-05-07T08:40:54.000Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>k神</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title></title>
    <link href="http://yoursite.com/2017/05/07/BP%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    <id>http://yoursite.com/2017/05/07/BP神经网络学习笔记/</id>
    <published>2017-05-07T08:40:26.000Z</published>
    <updated>2017-05-07T08:40:54.000Z</updated>
    
    <content type="html"><![CDATA[<hr>
<p>title: 《神经网络与深度学习》学习笔记<br>date: 2017-05-07 17:27:37</p>
<h2 id="tags-神经网络-深度学习"><a href="#tags-神经网络-深度学习" class="headerlink" title="tags:  神经网络,深度学习"></a>tags:  神经网络,深度学习</h2> <blockquote class="blockquote-center"><br> <footer style="textalign:center;"><br>康肃问曰：”汝亦知射乎？吾射不亦精乎？”翁曰：”无他， 但手熟尔。”<br> </footer></blockquote>

<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul>
<li>神经网络进行学习的基本前提  </li>
<li>神经网络两个假设与四个等式</li>
<li>实现  </li>
<li>超参数的选择   </li>
<li>存在的问题与优化点   </li>
<li>为什么神经网络能够『解决』『所有』问题</li>
</ul>
<h2 id="神经网络进行学习的基本前提"><a href="#神经网络进行学习的基本前提" class="headerlink" title="神经网络进行学习的基本前提"></a>神经网络进行学习的基本前提</h2><p>神经网络对输入样本点的学习，主要是根据不断修改权值与偏移值，保证输出结果朝着可预期的方向不断改善。为了了解整个过程，我们首先观察权值矩阵和偏移值矩阵的变化如何影响最终结果的输出。当我们对权值矩阵进行了一个微小的修改：<br><img src="http://bos.nj.bpc.baidu.com/v1/agroup/c27adf4d2967b7abc46b888637d38cd9b14cb1e6" alt="图片"><br>　　<br>这个修改首先会反应在该神经元的激活值上：<br><img src="http://bos.nj.bpc.baidu.com/v1/agroup/2732cac8ed3188e69b0e58017a55f80534833b3f" alt="图片"></p>
<p>这个激活值的变化会影响所有下一层邻接神经元的输出值，并最终反应到输出层的变化：<br>　　<br><img src="http://bos.nj.bpc.baidu.com/v1/agroup/cc5e412675ecc1f2d067434a25da6b7dac692f97" alt="图片"><br>　　<br>如果选中其中一条变化的路线，可以用公式描述整个过程：<br><img src="http://bos.nj.bpc.baidu.com/v1/agroup/db79325ca40ef3dd364c432abb0bebb0ff532207" alt="图片"></p>
<p>如果将中间多个隐藏层与不同路线的变化当做一个黑盒，可以用公式来描述整个过程：<br><img src="http://bos.nj.bpc.baidu.com/v1/agroup/66601d09675d412db941f059cffaf677a5cfbad0" alt="图片"></p>
<p>其中，<img src="http://bos.nj.bpc.baidu.com/v1/agroup/b04a43288772e799a5d58bff376f85298986071a" alt="图片">描述的是最终结果对wjk的敏感程度，该公式也可以根据链式法则展开获得完整的变化。<br>　　<br>上述变化过程，在整个神经网络训练过程中不断重复，通过对权值矩阵和偏倚值矩阵不断『轻微』的『试探』，刺激输出值进行一个『轻微』的反馈，根据反馈的优劣改变刺激方向与程度，寻找一个相对可行的优化『路线』，并最终到达一个『最优值』。重复这个过程，就能保证整个网络在数据的基础上正常的运作起来。如下图所示：<br><img src="http://bos.nj.bpc.baidu.com/v1/agroup/81242ff69b3e5a787a56a0363a69aff50b045484" alt="图片"></p>
<p>因此，问题的关键在于保证『轻微』的变化会给予输出结果『轻微』的变化，如果可以，我们希望这种变化『朴素』到仅仅与变化源头的神经元存在关联。<br><img src="http://bos.nj.bpc.baidu.com/v1/agroup/d441e4afa6758a7329621a90413ffd7fc9fdeada" alt="图片"><br>如上述公式所描述，最终结果的变化为权值与偏倚值值的线性变化，这样，我们就可以控制Delta值的系数，以此保证了输出值变化的可控程度。</p>
<p>在此基础上，我们考虑，如何来衡量『轻微』。<br><img src="http://bos.nj.bpc.baidu.com/v1/agroup/c2962ff2362a7a9e0aadf095545e99ed00310837" alt="图片"><br>『轻微』的含义是，变化源头的神经源微小的变化，不论中间隐藏层各个神经元如何变化，对输出也造成相同程度的变化。因此我们需要保证激活值的变化是先对『连续』的。<br><img src="http://bos.nj.bpc.baidu.com/v1/agroup/9443b29af17200ad396b44ef29ff82578048f7cc" alt="图片"><br>如果选择阶跃函数作为激活函数，这一个输入的轻微变化可能导致输出由0变成1，进而影响下一层所有邻接神经元，最终对输出造成极大的变化。<br><img src="http://bos.nj.bpc.baidu.com/v1/agroup/e6752f940062f1d87e7c4f80837c4dc393a4b024" alt="图片"><br>因而，在神经网络中，经常选择sigmoid function作为激活函数，这类神经元也被称为S型神经元。权值与偏倚值值的微小变化，同样反应在输出值的微小变化。</p>
<h2 id="神经网络两个假设与四个等式"><a href="#神经网络两个假设与四个等式" class="headerlink" title="神经网络两个假设与四个等式"></a>神经网络两个假设与四个等式</h2><h3 id="代价函数的两个假设"><a href="#代价函数的两个假设" class="headerlink" title="代价函数的两个假设"></a>代价函数的两个假设</h3><p>最简单的代价函数是用来衡量输出结果与真实结果间的一种误差程度，进而用来知道权值矩阵和偏倚值矩阵的优化方向。</p>
<p>平方代价函数：<br><img src="http://bos.nj.bpc.baidu.com/v1/agroup/53791aa683817780aa4155d2f699ed9c3ea8cc3f" alt="图片"></p>
<p>交叉熵代价函数：<br><img src="http://bos.nj.bpc.baidu.com/v1/agroup/3267fbde7976bd97cd434c8bf3206a6b1e442ba8" alt="图片"></p>
<p>关于代价函数的选择，有两个基本的假设作为前提。</p>
<h4 id="假设1"><a href="#假设1" class="headerlink" title="假设1"></a>假设1</h4><p><img src="http://bos.nj.bpc.baidu.com/v1/agroup/195549767f60fa780284ad58f6fd6a56bb307a17" alt="图片"><br>代价函数能够被写成上述表达形式，其中C为整体代价，Cx为单个训练样本点的代价。该假设存在的原因是在训练过程中，都是以单个样本点作为单位进行训练。</p>
<h4 id="假设2"><a href="#假设2" class="headerlink" title="假设2"></a>假设2</h4><p>第二条假设是代价函数能够被写成最终激活值的函数表示，如下所示：<br><img src="http://bos.nj.bpc.baidu.com/v1/agroup/2f0840b3ccffe18057c9621f118deb6d722cc516" alt="图片"><br>该假设存在的原因是只有激活值是直接与权值矩阵，偏倚值矩阵关联在一起，因此只有将代价函数与激活值关联在一起，才能对这两者优化方向进行判定。</p>
<h3 id="BP背后的四个基本等式"><a href="#BP背后的四个基本等式" class="headerlink" title="BP背后的四个基本等式"></a>BP背后的四个基本等式</h3><p>基于梯度下降算法的神经网络训练在上世纪八十年代就已经被提出，但是，优于weight与biases的梯度计算，需要考虑所有可能的路线，每条路线需要根据链式法则计算各个偏导值，这些偏导值存在大量的重复计算，当隐藏层数量与神经元数量增加，计算量将爆炸增长。可行的解决方案包括使用DP记录各个偏导的计算结果，以空间换时间的方式来降低计算时间，不过不知道为什么DP并没有应用在神经网络的训练中。<br><img src="http://bos.nj.bpc.baidu.com/v1/agroup/db79325ca40ef3dd364c432abb0bebb0ff532207" alt="图片"></p>
<p>后来，有大神提出了通过后向反馈将误差值逐层『平摊』到各个神经元上，而平摊考虑到某个神经元对下一层邻接神经元的『贡献量』，而『贡献量』关联了权值矩阵和偏倚值矩阵的梯度，这个梯度又被应用与梯度下降时目标的<strong><em>学习速率</em></strong>，整个过程只需要从后向前传播一次即可。这种方法极大降低了神经网络训练过程中的计算量，也标志这神经网络训练的实用化。</p>
<blockquote>
<p>提到学习速率这个指标，有些资料会说η就是学习速率。 η真正代表的是学习步长。在梯度下降训练中，η（步长）表示每次沿梯度最大方向下降一步的距离。真正的学习速率，应该是权值矩阵与偏倚值矩阵的梯度大小，梯度越大，代表学习速率越快。</p>
</blockquote>
<p>整个后向反馈神经网络的训练，就是将一个误差变量与各个偏导关联在一起，通过这个中间变量来完成偏导的获取，进而实现更新。这两者的关系，可以浓缩为四个基本等式：</p>
<p><img src="http://bos.nj.bpc.baidu.com/v1/agroup/bbf991c36852ffc444196228d51d618e7d6d87c2" alt="图片"></p>
<h4 id="BP1"><a href="#BP1" class="headerlink" title="BP1"></a>BP1</h4><p>方程一描述的是误差变量的定义，这个变量存在的目的是将反向传播过程中权值矩阵和偏倚值值矩阵的梯度联系起来。<br><img src="http://bos.nj.bpc.baidu.com/v1/agroup/21db3e1aa4287877c5d73fdd54c0fc4e65aa2b85" alt="图片"></p>
<h4 id="BP2"><a href="#BP2" class="headerlink" title="BP2"></a>BP2</h4><p>方程二描述的是反向传播过程中误差值的传播，从输出层的误差向前传播，并将误差逐层记录，推导过程如下：<br><img src="http://bos.nj.bpc.baidu.com/v1/agroup/2c0b4e278e37356fca9a3b70d0f797a1511c9eb2" alt="图片"></p>
<h4 id="BP3"><a href="#BP3" class="headerlink" title="BP3"></a>BP3</h4><p>方程三将误差变量与偏倚值矩阵的梯度关联在一起<br><img src="http://bos.nj.bpc.baidu.com/v1/agroup/1b5e331b60347a32d782d3d480e1da182adece01" alt="图片"></p>
<h4 id="BP4"><a href="#BP4" class="headerlink" title="BP4"></a>BP4</h4><p>方程四将误差变量与权重矩阵的梯度关联在一起<br><img src="http://bos.nj.bpc.baidu.com/v1/agroup/25acb007888a3ca38cd937b3f2f94b92c2c6c1df" alt="图片"></p>
<h2 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h2><p>两个假设和BP1-4指定了在神经网络训练过程中权值矩阵和偏倚值矩阵梯度的获取方式，通过类似DP的方式降低了求导的计算量。在原有的计算方法中，对weight或biases的偏导通过链式法则的方式计算，存在极大的重复计算量，特别是在节点规模和weight矩阵规模增大时将呈指数增长。</p>
<p><img src="http://bos.nj.bpc.baidu.com/v1/agroup/fb3436c0161c58556687a1749bc832b4adbbba75" alt="图片"><br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">SGD</span><span class="params">(self, train_data, epoll, mini_batch_size, eta, test_data = None)</span>:</span></div><div class="line">    <span class="keyword">if</span> test_data: n_test = len(test_data)</div><div class="line">    n = len(train_data)</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> xrange(epoll):</div><div class="line">        random.shuffle(train_data)</div><div class="line">        mini_batches = [train_data[j:j+mini_batch_size] <span class="keyword">for</span> j <span class="keyword">in</span> xrange(<span class="number">0</span>, n, mini_batch_size)]</div><div class="line">        <span class="keyword">for</span> mini_batch <span class="keyword">in</span> mini_batches:</div><div class="line">           self.update_mini_batch(mini_batch, eta)</div><div class="line"></div><div class="line">        <span class="keyword">if</span> test_data:</div><div class="line">            <span class="keyword">print</span> <span class="string">"Epoll &#123;0&#125;: &#123;1&#125; / &#123;2&#125;"</span>.format(i,self.evaluate(test_data),n_test)</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            <span class="keyword">print</span> <span class="string">"Epoll &#123;0&#125; complete"</span>.format(i)</div></pre></td></tr></table></figure></p>
<p>SGD是入口函数，主要完成训练样本的散列和分批计算，通过调用update_mini_batch，完成批数据的训练</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">backpop</span><span class="params">(self, x, y)</span>:</span></div><div class="line">    nndl_w = [np.zeros(np.shape(item)) <span class="keyword">for</span> item <span class="keyword">in</span> self.weights]</div><div class="line">    nndl_b = [np.zeros(np.shape(item)) <span class="keyword">for</span> item <span class="keyword">in</span> self.biases]</div><div class="line"></div><div class="line">    z, zs, activations = self.feedforword(x)</div><div class="line"></div><div class="line">    <span class="comment">#delta = self.cost_derivative(activations[-1], y) * sigmoid_prime(z)</span></div><div class="line">    delta = self.cost_derivative(activations[<span class="number">-1</span>], y)</div><div class="line">    nndl_b[<span class="number">-1</span>] = delta</div><div class="line">    nndl_w[<span class="number">-1</span>] = np.dot(delta, activations[<span class="number">-2</span>].transpose())</div><div class="line"></div><div class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> xrange(<span class="number">2</span>, self.num_layers):</div><div class="line">        delta = np.dot(self.weights[-l+<span class="number">1</span>].transpose(),delta) * sigmoid_prime(zs[-l])</div><div class="line">        nndl_b[-l] = delta</div><div class="line">        nndl_w[-l] = np.dot(delta, activations[-l<span class="number">-1</span>].transpose())</div><div class="line"></div><div class="line">    <span class="keyword">return</span> nndl_b, nndl_w</div></pre></td></tr></table></figure>
<p>backpop是在反向传播过程中，逐层计算误差变量的值，并根据BP3与BP4更新权值矩阵和偏倚值矩阵的梯度。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_mini_batch</span><span class="params">(self, mini_batch, eta)</span>:</span></div><div class="line">    nndl_w = [np.zeros(item.shape) <span class="keyword">for</span> item <span class="keyword">in</span> self.weights]</div><div class="line">    nndl_b = [np.zeros(item.shape) <span class="keyword">for</span> item <span class="keyword">in</span> self.biases]</div><div class="line"></div><div class="line">    <span class="keyword">for</span> x,y <span class="keyword">in</span> mini_batch:</div><div class="line">        b, w = self.backpop(x, y)</div><div class="line">        nndl_b = [b + nb <span class="keyword">for</span> b, nb <span class="keyword">in</span> zip(nndl_b, b)]</div><div class="line">        nndl_w = [w + nw <span class="keyword">for</span> w, nw <span class="keyword">in</span> zip(nndl_w, w)]</div><div class="line"></div><div class="line">    self.biases= [b - (eta/len(mini_batch))*nb <span class="keyword">for</span> b, nb <span class="keyword">in</span> zip(self.biases, nndl_b)]</div><div class="line">    self.weights = [w - (eta/len(mini_batch))*nw <span class="keyword">for</span> w, nw <span class="keyword">in</span> zip(self.weights, nndl_w)]</div></pre></td></tr></table></figure>
<p>update_mini_batch主要完成样本点的前向传播，后向反馈过程，并在后向反馈过程中更新权值矩阵和便宜矩阵。<br>其中：</p>
<ul>
<li>backpop完成后向反馈过程，并将误差变量记录 </li>
<li>weight和biases的更新使用梯度下降方式更新</li>
</ul>
<p><img src="http://bos.nj.bpc.baidu.com/v1/agroup/e6291b0709f95e6d0d1b83a3bf7f52d51b01eb10" alt="图片"></p>
<h2 id="超参数的选择"><a href="#超参数的选择" class="headerlink" title="超参数的选择"></a>超参数的选择</h2><h2 id="存在的问题与优化点"><a href="#存在的问题与优化点" class="headerlink" title="存在的问题与优化点"></a>存在的问题与优化点</h2><h2 id="为什么神经网络能够『解决』『所有』问题"><a href="#为什么神经网络能够『解决』『所有』问题" class="headerlink" title="为什么神经网络能够『解决』『所有』问题"></a>为什么神经网络能够『解决』『所有』问题</h2>]]></content>
    
    <summary type="html">
    
      &lt;hr&gt;
&lt;p&gt;title: 《神经网络与深度学习》学习笔记&lt;br&gt;date: 2017-05-07 17:27:37&lt;/p&gt;
&lt;h2 id=&quot;tags-神经网络-深度学习&quot;&gt;&lt;a href=&quot;#tags-神经网络-深度学习&quot; class=&quot;headerlink&quot; title=&quot;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>《Node.js学习笔记（一）：内存控制与垃圾回收》</title>
    <link href="http://yoursite.com/2016/12/01/node-gc-part1/"/>
    <id>http://yoursite.com/2016/12/01/node-gc-part1/</id>
    <published>2016-12-01T03:19:14.000Z</published>
    <updated>2016-12-09T04:00:49.000Z</updated>
    
    <content type="html"><![CDATA[ <blockquote class="blockquote-center">Node就是一群前端的自嗨</blockquote>



<p>&lt;-未完待续-&gt;</p>
<h2 id="V8的垃圾回收机制"><a href="#V8的垃圾回收机制" class="headerlink" title="V8的垃圾回收机制"></a>V8的垃圾回收机制</h2><h3 id="Scavenge算法"><a href="#Scavenge算法" class="headerlink" title="Scavenge算法"></a>Scavenge算法</h3><p>之前已经提到过，在V8将内存分为新生代和老生代两部分。而内存的分配主要集中在新生代部分，同时，新生代部分因为存活对象比较少，因此，新生代部分的垃圾回收主要使用scavenge算法。算法将新生代分成From区和To区，内存的分配主要是在From区中进行。当内存分配过程中allocationPtr指针到达From区结尾，就将触发垃圾回收机制。</p>
<p><div align="center"><br><img src="http://ohhpn0w2k.bkt.clouddn.com/20140725225453475.jpg" alt=""><br></div></p>
<blockquote>
<ol>
<li>可达性分析：即通过一系列称为GC ROOT的对象作为起点，从这些节点开始向下进行搜索，搜索走过的路径称为引用链,当一个对象到GC ROOT没有任何引用链，则证明该对象不可达。</li>
<li>一个等价约定：<a href="http://newhtml.net/v8-garbage-collection/" title="V8之旅：垃圾回收器" rel="external nofollow noopener noreferrer" target="_blank">如果一个对象可经由某个被定义为活跃对象的对象，通过某个指针链所访问，则它就是活跃的。其他的都被视为垃圾</a></li>
</ol>
</blockquote>
<h4 id="算法基本流程"><a href="#算法基本流程" class="headerlink" title="算法基本流程"></a>算法基本流程</h4><p>算法对应的伪码如下描述：<br><figure class="highlight php"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line">def scavenge():</div><div class="line">  swap(fromSpace, toSpace)</div><div class="line">  allocationPtr = toSpace.bottom</div><div class="line">  scanPtr = toSpace.bottom</div><div class="line"></div><div class="line">  <span class="keyword">for</span> i = <span class="number">0.</span>.len(roots):</div><div class="line">    root = roots[i]</div><div class="line">    <span class="keyword">if</span> inFromSpace(root):</div><div class="line">      rootCopy = copyObject(&amp;allocationPtr, root)</div><div class="line">      setForwardingAddress(root, rootCopy)</div><div class="line">      roots[i] = rootCopy</div><div class="line"></div><div class="line">  <span class="keyword">while</span> scanPtr &lt; allocationPtr:</div><div class="line">    obj = object at scanPtr</div><div class="line">    scanPtr += size(obj)</div><div class="line">    n = sizeInWords(obj)</div><div class="line">    <span class="keyword">for</span> i = <span class="number">0.</span>.n:</div><div class="line">      <span class="keyword">if</span> isPointer(obj[i]) <span class="keyword">and</span> not inOldSpace(obj[i]):</div><div class="line">        fromNeighbor = obj[i]</div><div class="line">        <span class="keyword">if</span> hasForwardingAddress(fromNeighbor):</div><div class="line">          toNeighbor = getForwardingAddress(fromNeighbor)</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">          toNeighbor = copyObject(&amp;allocationPtr, fromNeighbor)</div><div class="line">          setForwardingAddress(fromNeighbor, toNeighbor)</div><div class="line">        obj[i] = toNeighbor</div><div class="line"></div><div class="line">def copyObject(*allocationPtr, object):</div><div class="line">  copy = *allocationPtr</div><div class="line">  *allocationPtr += size(object)</div><div class="line">  memcpy(copy, object, size(object))</div><div class="line">  <span class="keyword">return</span> copy</div></pre></td></tr></table></figure></p>
<p>算法主要有下述几个步骤组成：</p>
<ul>
<li>基本环境值修改，如交换From/To区指针等信息</li>
<li>将GC ROOT根节点所有子节点从From区移动到To区</li>
<li>复制剩余存活节点<ul>
<li>第二步复制结束后，allocationPtr指向下一个可复制位置，scanPtr指向第一个开始复制的位置</li>
<li>对[scanPtr,allocationPtr)这段区间中的每个对象进行判断</li>
<li>如果是指针，并且不在老生代中<ul>
<li>如果已经移动到To区，直接修改指针指向的内存地址</li>
<li>否则，将指针指向的对象移动到To区，同时修改指针指向的内存地址</li>
</ul>
</li>
<li>重复整个流程，指导scanPtr = allocationPtr</li>
</ul>
</li>
</ul>
<p>整个流程其实类似一个广度优先搜索，从GC ROOT开始，一层一层地往下搜索，直到所有被引用的存活对象都被访问并且被移动到To区为止。</p>
<h4 id="写屏障"><a href="#写屏障" class="headerlink" title="写屏障"></a>写屏障</h4><p>上节讲到，在存活对象的遍历过程中，会根据GC ROOT判断某个对象是否存活。如果有个位于老生代的指针指向了新生代中某个对象，这时候如果需要去遍历整个老生代，在时间上的消耗将很大。V8的解决方案是在老生代指针指向新生代对象时设置一个写屏障。</p>
<ul>
<li>当一个新生代对象刚生成的时候，并不存在一个指向它的指针</li>
<li>当存在一个老生代对象指向它时，在写屏障中记录这个信息</li>
<li>当一个新生代对象晋升时，在写屏障中记录这个信息</li>
</ul>
<h3 id="Mark-Sweep-amp-Mark-Compact算法"><a href="#Mark-Sweep-amp-Mark-Compact算法" class="headerlink" title="Mark-Sweep&amp;Mark-Compact算法"></a>Mark-Sweep&amp;Mark-Compact算法</h3><p>​    Scavenge算法之所以在新生代垃圾回收中能够有效运行，很大程度上取决于新生代对象在一次垃圾回收后存活对象比较少。但是，老生代对象在每轮垃圾回收中都会有大量的存活对象，这时候如果使用Scavenge算法，将不可避免造成大量的内存移动，因此，V8中使用Mark-Sweep和Mark-Compact算法进行垃圾回收。</p>
<h4 id="标记算法"><a href="#标记算法" class="headerlink" title="标记算法"></a>标记算法</h4><blockquote>
<p>位图区：内存页还含有一个页头（包含一些元数据和标识信息）以及一个位图区（用以标记哪些对象是活跃的），位示图是利用二进制的一位来表示磁盘中一个块的作用情况，当其值为0 时表示对应盘块空闲；值为1时盘块已分配。磁盘上所有盘块都有一个二进制位与之对应，这样，由所有盘块所对应的位形成了一个集合称为位示图，位示图用磁盘块存放，称为位图块。</p>
</blockquote>
<p>在标记阶段，每个页都会有一个页图用来做标记的位图，位图中每一位代表页中一个字（注：一个字就是一个指针的大小，考虑到指针可能在一页中任何位子出现，）</p>
<p>标记清除和标记压缩算法的第一步都是标记存活对象，具体流程如下伪码所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line">markingDeque = []</div><div class="line">overflow = false</div><div class="line"></div><div class="line">def markHeap():</div><div class="line">  for root in roots:</div><div class="line">    mark(root)</div><div class="line"></div><div class="line">  do:</div><div class="line">    if overflow:</div><div class="line">      overflow = false</div><div class="line">      refillMarkingDeque()</div><div class="line"></div><div class="line">    while !markingDeque.isEmpty():</div><div class="line">      obj = markingDeque.pop()</div><div class="line">      setMarkBits(obj, BLACK)</div><div class="line">      for neighbor in neighbors(obj):</div><div class="line">        mark(neighbor)</div><div class="line">  while overflow</div><div class="line">    </div><div class="line"></div><div class="line">def mark(obj):</div><div class="line">  if markBits(obj) == WHITE:</div><div class="line">    setMarkBits(obj, GREY)</div><div class="line">    if markingDeque.isFull():</div><div class="line">      overflow = true</div><div class="line">    else:</div><div class="line">      markingDeque.push(obj)</div><div class="line"></div><div class="line">def refillMarkingDeque():</div><div class="line">  for each obj on heap:</div><div class="line">    if markBits(obj) == GREY:</div><div class="line">      markingDeque.push(obj)</div><div class="line">      if markingDeque.isFull():</div><div class="line">        overflow = true</div><div class="line">        return</div></pre></td></tr></table></figure>
<p>### </p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ul>
<li><a href="https://bugs.chromium.org/p/v8/issues/detail?id=847" rel="external nofollow noopener noreferrer" target="_blank">https://bugs.chromium.org/p/v8/issues/detail?id=847</a></li>
<li><a href="http://www.linuxidc.com/Linux/2015-03/115186.htm" rel="external nofollow noopener noreferrer" target="_blank">http://www.linuxidc.com/Linux/2015-03/115186.htm</a></li>
<li><a href="http://t.viewpoint.gr/node/deps/v8/src/heap.cc" rel="external nofollow noopener noreferrer" target="_blank">http://t.viewpoint.gr/node/deps/v8/src/heap.cc</a></li>
<li><a href="http://gold.xitu.io/entry/564ae48200b0d1db3385688e" rel="external nofollow noopener noreferrer" target="_blank">http://gold.xitu.io/entry/564ae48200b0d1db3385688e</a></li>
<li><a href="https://github.com/drewfish/node-tick-processor" rel="external nofollow noopener noreferrer" target="_blank">https://github.com/drewfish/node-tick-processor</a></li>
<li><a href="http://newhtml.net/v8-garbage-collection/" rel="external nofollow noopener noreferrer" target="_blank">http://newhtml.net/v8-garbage-collection/</a></li>
<li><a href="http://www.csdn.net/article/1970-01-01/2826316" rel="external nofollow noopener noreferrer" target="_blank">http://www.csdn.net/article/1970-01-01/2826316</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
       &lt;blockquote class=&quot;blockquote-center&quot;&gt;Node就是一群前端的自嗨&lt;/blockquote&gt;



&lt;p&gt;&amp;lt;-未完待续-&amp;gt;&lt;/p&gt;
&lt;h2 id=&quot;V8的垃圾回收机制&quot;&gt;&lt;a href=&quot;#V8的垃圾回收机制&quot; class=&quot;he
    
    </summary>
    
    
      <category term="node.js,v8,垃圾回收" scheme="http://yoursite.com/tags/node-js-v8-%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/"/>
    
  </entry>
  
  <entry>
    <title>序</title>
    <link href="http://yoursite.com/2016/11/24/helloworld/"/>
    <id>http://yoursite.com/2016/11/24/helloworld/</id>
    <published>2016-11-24T09:27:37.000Z</published>
    <updated>2016-11-28T07:48:50.000Z</updated>
    
    <content type="html"><![CDATA[<p> <blockquote class="blockquote-center"></blockquote></p>
<p><footer style="text-align:left;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;世界上有两件东西能够深深地震撼人们的心灵，一件是我们心中崇高的道德准则，另一件是我们头顶上灿烂的星空。</footer><br></p>
<p><footer style="text-align:center;font-weight:bold;color:red">过去</footer><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;引言其实和这篇序没什么关系，只不过是我很喜欢的一句话。曾经有一个大神大晚上赤裸上半身、抱着把吉他来213寝室，深情的说了这句我很喜欢的话，虽然我忘了上下文，虽然我觉得他单纯为了装逼，不过，我喜欢。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;到现在我还记得很多年前那个夏天的早晨，九点的教学楼到处弥漫着那个小岛上夏天特有的味道，慵懒的阳光和通宵打游戏后朦胧的睁不开的双眼。不为什么，只是感觉，那几年是人生中最好的时光，懒散、无虑，有大把大把的时间可以瘫在图书馆顶楼的沙发上思考自己想做什么，虽然一般除了抄作业就是看小说。我其实很喜欢小岛上的时光，我想念烧仙草、大开杯、海蛎煎、黄则和还有大径村那条小吃街上所有的小吃，还有那个我吃了好几年没变过的香菇鸡丁盖浇饭配纯牛奶。虽然养成了拖延癌晚期，但那里改变了我的一切。</p>
<a id="more"></a>
<p><footer style="text-align:center;font-weight:bold;color:red">现在</footer><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;一眨眼来帝都快一年了，一年前的这时候，我还在先研院的单间里通宵打游戏，那时候我做梦都想不到，现在会在这个离家一千多里的地方一个人学习、工作、生活。记得那时候刚下火车，我好奇的看了眼地铁口茫茫人海，琢磨了好久才想明白自己已经在一个完全陌生的地方，一个自己几个月前还坚定的认为打死也不来的地方，去面对一群没见过面的人。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;人生就是这么的奇妙。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;有的时候，拼命想去获取一个看似应该属于自己的东西，一个晃神老天就狠狠散了你一巴掌，然后缺心眼地嘲讽你的无能。命是个很操蛋的东西，不是你的东西，你怎么努力抢也抢不到，关键你还会觉得这真的是你的东西啊。就像很久以前，我就是想找个离家近点的学校，最后脸被啪啪啪地抽肿。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我一直觉得自己是个不爱折腾不思进取不求变化的人，偏偏误打误撞当了个程序员，当然不思进取可能有点过。作为一名非典型码农，你要我开篇写个Helloworld，诶，我觉得你是在污辱我情操，但凡有点底线的码农都不会跟风写个Helloworld，你写个HelloKitty都行，虽然码农并不是什么很丢脸的事。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;其实我费尽心思琢磨了好久，想写点带点灰色嘲讽的『序』来凸显自己的高逼格，以作为哥哥职业生涯的开端，然后才发现，自己已经不比当年的才思泉涌，上了年纪的人，思绪有点慢，那就酱紫吧。</p>
<p><footer style="text-align:center;font-weight:bold;color:red">未来</footer><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;多年前我第一次踏进大学校门的时候，就明白了一个道理。大家都说，没有人会知道自己的未来是怎样。其实这话不对。大家都知道自己的未来是什么样的，但是又不敢承认，也不知道这个未来要多久，有多难，这个未来和我预期的有多大的契合度，自己能不能在这个过程中坚持下来，又愿意付出多少。这事其实很残酷。然后，还有很多鸡汤会和你说，年轻人不要急，一步一步慢慢来。哔哔哔哔，如果年轻的时候，我一定会这么说，我不远千里跑到这人生地不熟的地方你叫我慢慢来，是你逗我还是想我逗你。不过想想现在都二十好几了，还是慢慢来吧。</p>
<p><footer style="text-align:center;font-weight:bold;color:red">后记</footer><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;大晚上的，居然有个小哥说我浪，诶，人如果不浪，那和咸鱼有什么区别？</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt; &lt;blockquote class=&quot;blockquote-center&quot;&gt;&lt;/blockquote&gt;&lt;/p&gt;
&lt;p&gt;&lt;footer style=&quot;text-align:left;&quot;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;世界上有两件东西能够深深地震撼人们的心灵，一件是我们心中崇高的道德准则，另一件是我们头顶上灿烂的星空。&lt;/footer&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;footer style=&quot;text-align:center;font-weight:bold;color:red&quot;&gt;过去&lt;/footer&gt;&lt;br&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;引言其实和这篇序没什么关系，只不过是我很喜欢的一句话。曾经有一个大神大晚上赤裸上半身、抱着把吉他来213寝室，深情的说了这句我很喜欢的话，虽然我忘了上下文，虽然我觉得他单纯为了装逼，不过，我喜欢。&lt;br&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;到现在我还记得很多年前那个夏天的早晨，九点的教学楼到处弥漫着那个小岛上夏天特有的味道，慵懒的阳光和通宵打游戏后朦胧的睁不开的双眼。不为什么，只是感觉，那几年是人生中最好的时光，懒散、无虑，有大把大把的时间可以瘫在图书馆顶楼的沙发上思考自己想做什么，虽然一般除了抄作业就是看小说。我其实很喜欢小岛上的时光，我想念烧仙草、大开杯、海蛎煎、黄则和还有大径村那条小吃街上所有的小吃，还有那个我吃了好几年没变过的香菇鸡丁盖浇饭配纯牛奶。虽然养成了拖延癌晚期，但那里改变了我的一切。&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
</feed>
