<!doctype html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">



<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">












  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css">


  <meta name="keywords" content="Hexo, NexT">




  


  <link rel="alternate" href="/atom.xml" title="proxukun" type="application/atom+xml">




  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.0">






<meta name="description" content="康肃问曰：”汝亦知射乎？吾射不亦精乎？”翁曰：”无他， 但手熟尔。”

目录
神经网络进行学习的基本前提  
神经网络两个假设与四个等式
实现  
超参数的选择   
存在的问题与优化点   
为什么神经网络能够『解决』『所有』问题

神经网络进行学习的基本前提神经网络对输入样本点的学习，主要是根据不断修改权值与偏倚值，保证输出结果朝着可预期的方向不断改善。为了了解整个过程，我们首先观察权值矩阵和">
<meta property="og:type" content="article">
<meta property="og:title" content="《神经网络与深度学习》学习笔记">
<meta property="og:url" content="http://yoursite.com/2017/05/07/《神经网络与深度学习》学习笔记/index.html">
<meta property="og:site_name" content="proxukun">
<meta property="og:description" content="康肃问曰：”汝亦知射乎？吾射不亦精乎？”翁曰：”无他， 但手熟尔。”

目录
神经网络进行学习的基本前提  
神经网络两个假设与四个等式
实现  
超参数的选择   
存在的问题与优化点   
为什么神经网络能够『解决』『所有』问题

神经网络进行学习的基本前提神经网络对输入样本点的学习，主要是根据不断修改权值与偏倚值，保证输出结果朝着可预期的方向不断改善。为了了解整个过程，我们首先观察权值矩阵和">
<meta property="og:image" content="http://bos.nj.bpc.baidu.com/v1/agroup/c27adf4d2967b7abc46b888637d38cd9b14cb1e6">
<meta property="og:image" content="http://bos.nj.bpc.baidu.com/v1/agroup/2732cac8ed3188e69b0e58017a55f80534833b3f">
<meta property="og:image" content="http://bos.nj.bpc.baidu.com/v1/agroup/cc5e412675ecc1f2d067434a25da6b7dac692f97">
<meta property="og:image" content="http://bos.nj.bpc.baidu.com/v1/agroup/db79325ca40ef3dd364c432abb0bebb0ff532207">
<meta property="og:image" content="http://bos.nj.bpc.baidu.com/v1/agroup/66601d09675d412db941f059cffaf677a5cfbad0">
<meta property="og:image" content="http://bos.nj.bpc.baidu.com/v1/agroup/b04a43288772e799a5d58bff376f85298986071a">
<meta property="og:image" content="http://bos.nj.bpc.baidu.com/v1/agroup/81242ff69b3e5a787a56a0363a69aff50b045484">
<meta property="og:image" content="http://bos.nj.bpc.baidu.com/v1/agroup/d441e4afa6758a7329621a90413ffd7fc9fdeada">
<meta property="og:image" content="http://bos.nj.bpc.baidu.com/v1/agroup/c2962ff2362a7a9e0aadf095545e99ed00310837">
<meta property="og:image" content="http://bos.nj.bpc.baidu.com/v1/agroup/9443b29af17200ad396b44ef29ff82578048f7cc">
<meta property="og:image" content="http://bos.nj.bpc.baidu.com/v1/agroup/e6752f940062f1d87e7c4f80837c4dc393a4b024">
<meta property="og:image" content="http://bos.nj.bpc.baidu.com/v1/agroup/53791aa683817780aa4155d2f699ed9c3ea8cc3f">
<meta property="og:image" content="http://bos.nj.bpc.baidu.com/v1/agroup/3267fbde7976bd97cd434c8bf3206a6b1e442ba8">
<meta property="og:image" content="http://bos.nj.bpc.baidu.com/v1/agroup/195549767f60fa780284ad58f6fd6a56bb307a17">
<meta property="og:image" content="http://bos.nj.bpc.baidu.com/v1/agroup/2f0840b3ccffe18057c9621f118deb6d722cc516">
<meta property="og:image" content="http://bos.nj.bpc.baidu.com/v1/agroup/db79325ca40ef3dd364c432abb0bebb0ff532207">
<meta property="og:image" content="http://bos.nj.bpc.baidu.com/v1/agroup/bbf991c36852ffc444196228d51d618e7d6d87c2">
<meta property="og:image" content="http://bos.nj.bpc.baidu.com/v1/agroup/21db3e1aa4287877c5d73fdd54c0fc4e65aa2b85">
<meta property="og:image" content="http://bos.nj.bpc.baidu.com/v1/agroup/2c0b4e278e37356fca9a3b70d0f797a1511c9eb2">
<meta property="og:image" content="http://bos.nj.bpc.baidu.com/v1/agroup/1b5e331b60347a32d782d3d480e1da182adece01">
<meta property="og:image" content="http://bos.nj.bpc.baidu.com/v1/agroup/25acb007888a3ca38cd937b3f2f94b92c2c6c1df">
<meta property="og:image" content="http://bos.nj.bpc.baidu.com/v1/agroup/fb3436c0161c58556687a1749bc832b4adbbba75">
<meta property="og:image" content="http://bos.nj.bpc.baidu.com/v1/agroup/e6291b0709f95e6d0d1b83a3bf7f52d51b01eb10">
<meta property="og:image" content="http://bos.nj.bpc.baidu.com/v1/agroup/3a3c44fffbec4d4e291e60d5dd0a16d7c6e6d25a">
<meta property="og:image" content="http://bos.nj.bpc.baidu.com/v1/agroup/48a4d1e7669faab52952d21c61866ca4f3992fa2">
<meta property="og:image" content="http://bos.nj.bpc.baidu.com/v1/agroup/477ac74af20d3256f821a7e528e5eef043edec7e">
<meta property="og:image" content="http://bos.nj.bpc.baidu.com/v1/agroup/6e298f0152ea10ace55de6eae307d948fb9aded0">
<meta property="og:image" content="http://bos.nj.bpc.baidu.com/v1/agroup/9cdbb2d1057f6db17844dc2e26df81fe6e8d8786">
<meta property="og:image" content="http://bos.nj.bpc.baidu.com/v1/agroup/890bcc6957f1ffc06b9dbe6780249f5cf452619c">
<meta property="og:image" content="http://bos.nj.bpc.baidu.com/v1/agroup/5246e3b8658b3e5cdf78fc364b098e8ab24c571a">
<meta property="og:image" content="http://bos.nj.bpc.baidu.com/v1/agroup/212e2239282befc20995b985623244eb7fa9ab48">
<meta property="og:image" content="http://bos.nj.bpc.baidu.com/v1/agroup/4e81bda549d68ba47b185714c2d25d4b0cbe0734">
<meta property="og:image" content="http://bos.nj.bpc.baidu.com/v1/agroup/a8a197341d15332a836f8d59c2966889dacc0336">
<meta property="og:image" content="http://bos.nj.bpc.baidu.com/v1/agroup/4ca3f8788b1383457dca8243ae834641db31c125">
<meta property="og:image" content="http://bos.nj.bpc.baidu.com/v1/agroup/1cbad3be0cab9ae60288f0c444c188b7f1e7f90c">
<meta property="og:image" content="http://bos.nj.bpc.baidu.com/v1/agroup/8fcdfb2dd61dfa593ec5ba352dbb74f40fcb2574">
<meta property="og:image" content="http://bos.nj.bpc.baidu.com/v1/agroup/28900731946af6a92c5a0e5160c75e241082f1a3">
<meta property="og:image" content="http://bos.nj.bpc.baidu.com/v1/agroup/62f63c1c18226e597e97acca2b710b1ffd77962f">
<meta property="og:image" content="http://bos.nj.bpc.baidu.com/v1/agroup/6f8535a8acbc88386202673b3c0852a531173e7a">
<meta property="og:image" content="http://bos.nj.bpc.baidu.com/v1/agroup/8c808ffad2239dd86a6fcbe0bd9940e8de9d9e24">
<meta property="og:image" content="http://bos.nj.bpc.baidu.com/v1/agroup/e7eb8d14dd28070c157bb66446c1a99bc74907d8">
<meta property="og:image" content="http://bos.nj.bpc.baidu.com/v1/agroup/a1bbab233406da2157f9ec2514836d5ebf4f6cd8">
<meta property="og:image" content="http://mmbiz.qpic.cn/mmbiz/58FUuNaBUjrcxwHE3MQxbaWKZgIr4VicavdQPVbFC5gAk6ClA1GkQJyEe9URv51geiaibQ5dDTgR7KgiaxsPw6afSw/0?wx_fmt=gif&tp=webp&wxfrom=5&wx_lazy=1">
<meta property="og:image" content="http://bos.nj.bpc.baidu.com/v1/agroup/5e47548e2052dc9de1fb5feda55a1b294eb10ff5">
<meta property="og:image" content="http://bos.nj.bpc.baidu.com/v1/agroup/ceb016eca512dfa576f384b6988cf06cbdc856fa">
<meta property="og:image" content="http://bos.nj.bpc.baidu.com/v1/agroup/8473e532e2d1f8eb7a08c6c912d460a510ab92a7">
<meta property="og:image" content="http://bos.nj.bpc.baidu.com/v1/agroup/6f34e2901e35670e82061623df6c82f679bc1b06">
<meta property="og:image" content="http://bos.nj.bpc.baidu.com/v1/agroup/2c1de2e0b8af6c515a3c131d77f9e71cb5108d9b">
<meta property="og:image" content="http://bos.nj.bpc.baidu.com/v1/agroup/1e5b091b6e9f6df72d8da0ed4fedef37f6501a4a">
<meta property="og:image" content="http://bos.nj.bpc.baidu.com/v1/agroup/f938a71c2fb21de7e377496f57fe2de51de7c631">
<meta property="og:updated_time" content="2017-05-08T09:29:38.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="《神经网络与深度学习》学习笔记">
<meta name="twitter:description" content="康肃问曰：”汝亦知射乎？吾射不亦精乎？”翁曰：”无他， 但手熟尔。”

目录
神经网络进行学习的基本前提  
神经网络两个假设与四个等式
实现  
超参数的选择   
存在的问题与优化点   
为什么神经网络能够『解决』『所有』问题

神经网络进行学习的基本前提神经网络对输入样本点的学习，主要是根据不断修改权值与偏倚值，保证输出结果朝着可预期的方向不断改善。为了了解整个过程，我们首先观察权值矩阵和">
<meta name="twitter:image" content="http://bos.nj.bpc.baidu.com/v1/agroup/c27adf4d2967b7abc46b888637d38cd9b14cb1e6">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    sidebar: {"position":"left","display":"post"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '6356875390790665000',
      author: '主编大人'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2017/05/07/《神经网络与深度学习》学习笔记/">





  <title> 《神经网络与深度学习》学习笔记 | proxukun </title>
</head>

<body itemscope="" itemtype="http://schema.org/WebPage" lang="zh-Hans">

  



  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "//hm.baidu.com/hm.js?a0a4930e55e55530ff0bab49e40d2f6e";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>








  
  
    
  

  <div class="container one-collumn sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">proxukun</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
  <p class="site-subtitle"></p>
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br>
            
            关于
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup">
 <span class="search-icon fa fa-search"></span>
 <input type="text" id="local-search-input">
 <div id="local-search-result"></div>
 <span class="popup-btn-close">close</span>
</div>


    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal " itemscope="" itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/05/07/《神经网络与深度学习》学习笔记/">

  <span style="display:none" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
    <meta itemprop="name" content="k神">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/uploads/avatar.png">
  </span>

  <span style="display:none" itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="proxukun">
    <span style="display:none" itemprop="logo" itemscope="" itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="proxukun" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                《神经网络与深度学习》学习笔记
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-05-07T21:08:34+08:00">
                2017-05-07
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <a href="/2017/05/07/《神经网络与深度学习》学习笔记/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2017/05/07/《神经网络与深度学习》学习笔记/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          

          
          
             <span id="/2017/05/07/《神经网络与深度学习》学习笔记/" class="leancloud_visitors" data-flag-title="《神经网络与深度学习》学习笔记">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               <span class="post-meta-item-text">阅读次数 </span>
               <span class="leancloud-visitors-count"></span>
              </span>
          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        <blockquote class="blockquote-center"><br><footer style="textalign:center;"><br>康肃问曰：”汝亦知射乎？吾射不亦精乎？”翁曰：”无他， 但手熟尔。”<br></footer></blockquote>

<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul>
<li>神经网络进行学习的基本前提  </li>
<li>神经网络两个假设与四个等式</li>
<li>实现  </li>
<li>超参数的选择   </li>
<li>存在的问题与优化点   </li>
<li>为什么神经网络能够『解决』『所有』问题</li>
</ul>
<h2 id="神经网络进行学习的基本前提"><a href="#神经网络进行学习的基本前提" class="headerlink" title="神经网络进行学习的基本前提"></a>神经网络进行学习的基本前提</h2><p>神经网络对输入样本点的学习，主要是根据不断修改权值与偏倚值，保证输出结果朝着可预期的方向不断改善。为了了解整个过程，我们首先观察权值矩阵和偏倚值矩阵的变化如何影响最终结果的输出。当我们对权值矩阵进行了一个微小的修改：<br><img src="http://bos.nj.bpc.baidu.com/v1/agroup/c27adf4d2967b7abc46b888637d38cd9b14cb1e6" alt="图片"><br>　<br><a id="more"></a>　<br>这个修改首先会反应在该神经元的激活值上：<br><img src="http://bos.nj.bpc.baidu.com/v1/agroup/2732cac8ed3188e69b0e58017a55f80534833b3f" alt="图片"></p>
<p>这个激活值的变化会影响所有下一层邻接神经元的激活值，并最终反应到输出层的变化：<br>　　<br><img src="http://bos.nj.bpc.baidu.com/v1/agroup/cc5e412675ecc1f2d067434a25da6b7dac692f97" alt="图片"><br>　　<br>如果选中其中一条变化的路线，可以用公式描述整个过程：<br><img src="http://bos.nj.bpc.baidu.com/v1/agroup/db79325ca40ef3dd364c432abb0bebb0ff532207" alt="图片"></p>
<p>如果将中间多个隐藏层与不同路线的变化当做一个黑盒，可以用公式来描述整个过程：<br><img src="http://bos.nj.bpc.baidu.com/v1/agroup/66601d09675d412db941f059cffaf677a5cfbad0" alt="图片"></p>
<p>其中，<img src="http://bos.nj.bpc.baidu.com/v1/agroup/b04a43288772e799a5d58bff376f85298986071a" alt="图片">描述的是最终结果对wjk的敏感程度，该公式也可以根据链式法则+穷举所有路径，展开获得完整的变化。</p>
<p>上述变化过程，在整个神经网络训练过程中不断重复，通过对权值矩阵和偏倚值矩阵不断『轻微』的『试探』，刺激输出值进行一个『轻微』的反馈，根据反馈的优劣改变刺激方向与程度，寻找一个相对可行的优化『路线』，并最终到达一个『最优值』。重复这个过程，就能保证整个网络在数据的基础上正常的运作起来。如下图所示：<br><img src="http://bos.nj.bpc.baidu.com/v1/agroup/81242ff69b3e5a787a56a0363a69aff50b045484" alt="图片"></p>
<p>因此，问题的关键在于保证『轻微』的变化会给予输出结果『轻微』的变化，如果可以，我们希望这种变化『朴素』到仅仅与变化源头的神经元存在关联。<br><img src="http://bos.nj.bpc.baidu.com/v1/agroup/d441e4afa6758a7329621a90413ffd7fc9fdeada" alt="图片"><br>如上述公式所描述，最终结果的变化为权值与偏倚值变化的线性变化，这样，我们就可以控制Delta值的系数，以此保证了输出值变化的可控程度。</p>
<p>在此基础上，我们考虑，如何来衡量『轻微』。<br><img src="http://bos.nj.bpc.baidu.com/v1/agroup/c2962ff2362a7a9e0aadf095545e99ed00310837" alt="图片"><br>『轻微』的含义是，变化源头的神经源微小的变化，不论中间隐藏层各个神经元如何变化，对输出也造成相同程度的变化。因此我们需要保证激活值的变化是相对『连续』的。<br><img src="http://bos.nj.bpc.baidu.com/v1/agroup/9443b29af17200ad396b44ef29ff82578048f7cc" alt="图片"><br>如果选择阶跃函数作为激活函数，这一个输入的轻微变化可能导致输出由0变成1，进而影响下一层所有邻接神经元，最终对输出造成极大的变化。<br><img src="http://bos.nj.bpc.baidu.com/v1/agroup/e6752f940062f1d87e7c4f80837c4dc393a4b024" alt="图片"><br>因而，在神经网络中，经常选择sigmoid function作为激活函数，这类神经元也被称为S型神经元。权值与偏倚值值的微小变化，同样反应在输出值的微小变化。</p>
<h2 id="神经网络两个假设与四个等式"><a href="#神经网络两个假设与四个等式" class="headerlink" title="神经网络两个假设与四个等式"></a>神经网络两个假设与四个等式</h2><h3 id="代价函数的两个假设"><a href="#代价函数的两个假设" class="headerlink" title="代价函数的两个假设"></a>代价函数的两个假设</h3><p>最简单的代价函数是用来衡量输出结果与真实结果间的一种误差程度，进而用来知道权值矩阵和偏倚值矩阵的优化方向。</p>
<p>最小二乘代价函数：<br><img src="http://bos.nj.bpc.baidu.com/v1/agroup/53791aa683817780aa4155d2f699ed9c3ea8cc3f" alt="图片"></p>
<p>交叉熵代价函数：<br><img src="http://bos.nj.bpc.baidu.com/v1/agroup/3267fbde7976bd97cd434c8bf3206a6b1e442ba8" alt="图片"></p>
<p>关于代价函数的选择，有两个基本的假设作为前提。</p>
<h4 id="假设1"><a href="#假设1" class="headerlink" title="假设1"></a>假设1</h4><p><img src="http://bos.nj.bpc.baidu.com/v1/agroup/195549767f60fa780284ad58f6fd6a56bb307a17" alt="图片"><br>代价函数能够被写成上述表达形式，其中C为整体代价，Cx为单个训练样本点的代价。该假设存在的原因是在训练过程中，都是以单个样本点作为单位进行训练。</p>
<h4 id="假设2"><a href="#假设2" class="headerlink" title="假设2"></a>假设2</h4><p>第二条假设是代价函数能够被写成最终激活值的函数表示，如下所示：<br><img src="http://bos.nj.bpc.baidu.com/v1/agroup/2f0840b3ccffe18057c9621f118deb6d722cc516" alt="图片"><br>该假设存在的原因是只有激活值是直接与权值矩阵，偏倚值矩阵关联在一起，因此只有将代价函数与激活值关联在一起，才能对这两者优化方向进行判定。</p>
<h3 id="BP背后的四个基本等式"><a href="#BP背后的四个基本等式" class="headerlink" title="BP背后的四个基本等式"></a>BP背后的四个基本等式</h3><p>基于梯度下降算法的神经网络训练在上世纪八十年代就已经被提出，但是，由于weight与biases的梯度计算，需要考虑所有可能的路线，每条路线需要根据链式法则计算各个偏导值，这些偏导值存在大量的重复计算，当隐藏层数量与神经元数量增加，计算量将爆炸增长。可行的解决方案包括使用DP记录各个偏导的计算结果，以空间换时间的方式来降低计算时间，不过不知道为什么DP并没有应用在神经网络的训练中。<br><img src="http://bos.nj.bpc.baidu.com/v1/agroup/db79325ca40ef3dd364c432abb0bebb0ff532207" alt="图片"></p>
<p>后来，有大神提出了通过后向反馈将误差值逐层『平摊』到各个神经元上，而平摊考虑到某个神经元对下一层邻接神经元的『贡献量』，而『贡献量』关联了权值矩阵和偏倚值矩阵的梯度，这个梯度又被应用与梯度下降时目标的<strong><em>学习速率</em></strong>，整个过程只需要从后向前传播一次即可。这种方法极大降低了神经网络训练过程中的计算量，也标志这神经网络训练的实用化。</p>
<blockquote>
<p>提到学习速率这个指标，有些资料会说η就是学习速率。 η真正代表的是学习步长。在梯度下降训练中，η（步长）表示每次沿梯度最大方向下降一步的距离。真正的学习速率，应该是权值矩阵与偏倚值矩阵变化的速度，可以简单认为是梯度的大小，梯度越大，代表当前的学习速率越快。</p>
</blockquote>
<p>整个后向反馈神经网络的训练，就是将一个误差变量与各个偏导关联在一起，通过这个中间变量来完成偏导的获取，进而实现更新。这两者的关系，可以浓缩为四个基本等式：</p>
<p><img src="http://bos.nj.bpc.baidu.com/v1/agroup/bbf991c36852ffc444196228d51d618e7d6d87c2" alt="图片"></p>
<h4 id="BP1"><a href="#BP1" class="headerlink" title="BP1"></a>BP1</h4><p>方程一描述的是误差变量的定义，该变量是整体误差平摊到各个子神经元的『贡献』总量的一个度量，这个变量存在的目的是将反向传播过程中权值矩阵和偏倚值值矩阵的梯度值联系起来。<br><img src="http://bos.nj.bpc.baidu.com/v1/agroup/21db3e1aa4287877c5d73fdd54c0fc4e65aa2b85" alt="图片"></p>
<h4 id="BP2"><a href="#BP2" class="headerlink" title="BP2"></a>BP2</h4><p>方程二描述的是反向传播过程中误差值的传播，从输出层的误差向前传播，并将误差逐层记录。该等式的存在，表明了误差变量的递推关系，前一层可以根据后一层的误差计算出来，推导过程如下：<br><img src="http://bos.nj.bpc.baidu.com/v1/agroup/2c0b4e278e37356fca9a3b70d0f797a1511c9eb2" alt="图片"></p>
<h4 id="BP3"><a href="#BP3" class="headerlink" title="BP3"></a>BP3</h4><p>方程三将误差变量与偏倚值矩阵的梯度关联在一起<br><img src="http://bos.nj.bpc.baidu.com/v1/agroup/1b5e331b60347a32d782d3d480e1da182adece01" alt="图片"></p>
<h4 id="BP4"><a href="#BP4" class="headerlink" title="BP4"></a>BP4</h4><p>方程四将误差变量与权重矩阵的梯度关联在一起<br><img src="http://bos.nj.bpc.baidu.com/v1/agroup/25acb007888a3ca38cd937b3f2f94b92c2c6c1df" alt="图片"></p>
<p>通过上述的四个等式，我们可以将梯度值的获取由链式法则的计算修改为误差传播的方式来获取。</p>
<h2 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h2><p>两个假设和BP1-4指定了在神经网络训练过程中权值矩阵和偏倚值矩阵梯度的获取方式，通过类似DP的方式降低了求导的计算量，具体计算过程如下描述：</p>
<p><img src="http://bos.nj.bpc.baidu.com/v1/agroup/fb3436c0161c58556687a1749bc832b4adbbba75" alt="图片"><br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">SGD</span><span class="params">(self, train_data, epoll, mini_batch_size, eta, test_data = None)</span>:</span></div><div class="line">    <span class="keyword">if</span> test_data: n_test = len(test_data)</div><div class="line">    n = len(train_data)</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> xrange(epoll):</div><div class="line">        random.shuffle(train_data)</div><div class="line">        mini_batches = [train_data[j:j+mini_batch_size] <span class="keyword">for</span> j <span class="keyword">in</span> xrange(<span class="number">0</span>, n, mini_batch_size)]</div><div class="line">        <span class="keyword">for</span> mini_batch <span class="keyword">in</span> mini_batches:</div><div class="line">           self.update_mini_batch(mini_batch, eta)</div><div class="line"></div><div class="line">        <span class="keyword">if</span> test_data:</div><div class="line">            <span class="keyword">print</span> <span class="string">"Epoll &#123;0&#125;: &#123;1&#125; / &#123;2&#125;"</span>.format(i,self.evaluate(test_data),n_test)</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            <span class="keyword">print</span> <span class="string">"Epoll &#123;0&#125; complete"</span>.format(i)</div></pre></td></tr></table></figure></p>
<p>SGD是入口函数，主要完成训练样本的随机散列和分批计算，通过调用update_mini_batch，完成批数据的训练</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">backpop</span><span class="params">(self, x, y)</span>:</span></div><div class="line">    nndl_w = [np.zeros(np.shape(item)) <span class="keyword">for</span> item <span class="keyword">in</span> self.weights]</div><div class="line">    nndl_b = [np.zeros(np.shape(item)) <span class="keyword">for</span> item <span class="keyword">in</span> self.biases]</div><div class="line"></div><div class="line">    z, zs, activations = self.feedforword(x)</div><div class="line"></div><div class="line">    <span class="comment">#delta = self.cost_derivative(activations[-1], y) * sigmoid_prime(z)</span></div><div class="line">    delta = self.cost_derivative(activations[<span class="number">-1</span>], y)</div><div class="line">    nndl_b[<span class="number">-1</span>] = delta</div><div class="line">    nndl_w[<span class="number">-1</span>] = np.dot(delta, activations[<span class="number">-2</span>].transpose())</div><div class="line"></div><div class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> xrange(<span class="number">2</span>, self.num_layers):</div><div class="line">        delta = np.dot(self.weights[-l+<span class="number">1</span>].transpose(),delta) * sigmoid_prime(zs[-l])</div><div class="line">        nndl_b[-l] = delta</div><div class="line">        nndl_w[-l] = np.dot(delta, activations[-l<span class="number">-1</span>].transpose())</div><div class="line"></div><div class="line">    <span class="keyword">return</span> nndl_b, nndl_w</div></pre></td></tr></table></figure>
<p>backpop是在反向传播过程中，逐层计算误差变量的值，并根据BP3与BP4更新权值矩阵和偏倚值矩阵的梯度。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_mini_batch</span><span class="params">(self, mini_batch, eta)</span>:</span></div><div class="line">    nndl_w = [np.zeros(item.shape) <span class="keyword">for</span> item <span class="keyword">in</span> self.weights]</div><div class="line">    nndl_b = [np.zeros(item.shape) <span class="keyword">for</span> item <span class="keyword">in</span> self.biases]</div><div class="line"></div><div class="line">    <span class="keyword">for</span> x,y <span class="keyword">in</span> mini_batch:</div><div class="line">        b, w = self.backpop(x, y)</div><div class="line">        nndl_b = [b + nb <span class="keyword">for</span> b, nb <span class="keyword">in</span> zip(nndl_b, b)]</div><div class="line">        nndl_w = [w + nw <span class="keyword">for</span> w, nw <span class="keyword">in</span> zip(nndl_w, w)]</div><div class="line"></div><div class="line">    self.biases= [b - (eta/len(mini_batch))*nb <span class="keyword">for</span> b, nb <span class="keyword">in</span> zip(self.biases, nndl_b)]</div><div class="line">    self.weights = [w - (eta/len(mini_batch))*nw <span class="keyword">for</span> w, nw <span class="keyword">in</span> zip(self.weights, nndl_w)]</div></pre></td></tr></table></figure>
<p>update_mini_batch主要完成样本点的前向传播，后向反馈过程，并在后向反馈过程中更新权值矩阵和偏倚值矩阵。<br>其中：</p>
<ul>
<li>backpop完成后向反馈过程，并将误差变量记录 </li>
<li>weight和biases的更新使用梯度下降方式更新</li>
</ul>
<p><img src="http://bos.nj.bpc.baidu.com/v1/agroup/e6291b0709f95e6d0d1b83a3bf7f52d51b01eb10" alt="图片"></p>
<h2 id="超参数的选择"><a href="#超参数的选择" class="headerlink" title="超参数的选择"></a>超参数的选择</h2><p>超参数主要包括4种</p>
<ul>
<li>批训练样本数量   </li>
<li>训练迭代次数 </li>
<li>步长 </li>
<li>正则化比率（lamda）</li>
</ul>
<p>参数的选择有很多启发式规则，总结如下</p>
<ul>
<li>在训练初期，减小问题与样本规模、难度 </li>
<li>快速试错，快速反馈 </li>
<li>更频繁的监控结果，减少监控需要的计算数量</li>
</ul>
<h2 id="存在的问题与优化点"><a href="#存在的问题与优化点" class="headerlink" title="存在的问题与优化点"></a>存在的问题与优化点</h2><p>网络训练过程中，最终要的有两点，训练时的学习速率和过拟合的抑制，前者保证网络能够更快的获取结果，特别是在训练初期，对超超速调整时；后者主要衡量训练出来的模型是否很好的抽象出一般性的通用规则。</p>
<h3 id="学习速率的优化"><a href="#学习速率的优化" class="headerlink" title="学习速率的优化"></a>学习速率的优化</h3><p>之前已经提到过，学习速率指的是权值矩阵和偏倚值矩阵朝着最优值方向更新的速度，这个速度代表这算法收敛到一个最优模型所花费的时间，因此这个速率是很重要的。</p>
<h4 id="学习速率慢的真正原因"><a href="#学习速率慢的真正原因" class="headerlink" title="学习速率慢的真正原因"></a>学习速率慢的真正原因</h4><p>在SGD中，初始权值矩阵和偏倚值矩阵的初始值是随机的，可以这么认为，在三维空间中，初始点的位置是随机的，我们将从一个随机的位置开始使用梯度下降进行最优值的搜寻。<br><img src="http://bos.nj.bpc.baidu.com/v1/agroup/3a3c44fffbec4d4e291e60d5dd0a16d7c6e6d25a" alt="图片"></p>
<ul>
<li>当我们的初始值w=0.6,b=0.9时，学习速率的变化，特别是在初始阶段的变化比较迅速，在逐渐接近最优值后，速度开始放缓。</li>
</ul>
<p><img src="http://bos.nj.bpc.baidu.com/v1/agroup/48a4d1e7669faab52952d21c61866ca4f3992fa2" alt="图片"></p>
<ul>
<li>当我们的初始值w=2.0,b=2.0时，学习速率的变化在初始阶段特别缓慢，学习一段时间后，才开始加速整个学习过程。</li>
</ul>
<p>考虑到我们在日常生活中面对的情景，如果面对一个 完全不熟悉的东西，特别是在误差特别明显的情况下，我们应该能够更快的调整并极大降低误差，这是我们对学习这件事的本能。<br>但是在上述SGD的学习过程中，表现和我们的本能完全相反，那学习速率缓慢的真正原因是？<br>下图是SGD中权值矩阵的更新方式<br><img src="http://bos.nj.bpc.baidu.com/v1/agroup/477ac74af20d3256f821a7e528e5eef043edec7e" alt="图片"><br>其中最重要的一项是w的梯度<br><img src="http://bos.nj.bpc.baidu.com/v1/agroup/6e298f0152ea10ace55de6eae307d948fb9aded0" alt="图片"><br>其中有一项是激活函数的偏导，根据之前的描述，我们一直选择sigmoid函数作为激活函数，观察sigmoid的函数曲线<br><img src="http://bos.nj.bpc.baidu.com/v1/agroup/9cdbb2d1057f6db17844dc2e26df81fe6e8d8786" alt="图片"></p>
<ul>
<li>sigmoid函数在x&lt;&lt;0时，导数约等于0 </li>
<li>sigmoid函数在x&gt;&gt;0时，导数约等于0</li>
</ul>
<p>这告诉我们，如果激活函数选择为sigmoid function，那么，在误差很大和误差很小的情况下，导数都约等于0，进而造成w和b的梯度接近0，这就是最小二乘的代价函数+sigmoid函数组合时造成学习速率更新慢的根本原因。</p>
<h4 id="交叉熵解决方案"><a href="#交叉熵解决方案" class="headerlink" title="交叉熵解决方案"></a>交叉熵解决方案</h4><p>所以，面对学习速率慢的问题，解决方案就是取消梯度中sigmoid函数导数这一项，但是，像sigmoid或者tanh这样的激活函数，具有很多其他函数不具备的优点，因此修改激活函数并非最优的解决方案。</p>
<p>取代的解决方案是修改最小二乘来衡量代价函数，使用交叉熵代价函数<br><img src="http://bos.nj.bpc.baidu.com/v1/agroup/890bcc6957f1ffc06b9dbe6780249f5cf452619c" alt="图片"><br>关于交叉熵的推到过程，最根本的想法是取消梯度中的导数，然后使用积分方法推到出。<br>至于交叉熵代表的含义，涉及到信息论中的内容，大概解释是，交叉熵衡量了一种混乱的程度，如果这种混乱程度能够降低到一个最小值，这个最小值代表了当前环境一种稳定的情况。<br>但选择了交叉熵代价函数和sigmoid函数的组合后，w的梯度变为<br><img src="http://bos.nj.bpc.baidu.com/v1/agroup/5246e3b8658b3e5cdf78fc364b098e8ab24c571a" alt="图片"><br>可以发现，激活函数的偏导项已经取消，同样对b的梯度公式也取消了导数。</p>
<h5 id="关于代价函数的修改对算法的影响"><a href="#关于代价函数的修改对算法的影响" class="headerlink" title="关于代价函数的修改对算法的影响"></a>关于代价函数的修改对算法的影响</h5><p>首先考虑，在BP1-4中，只有BP1直接与代价函数C存在关联，所以，整个算法中需要修改的仅仅是前向传播结束，后向反馈开始，整体误差的计算公式会发生变化。<br><img src="http://bos.nj.bpc.baidu.com/v1/agroup/212e2239282befc20995b985623244eb7fa9ab48" alt="图片"><br>其他部分还是和原来一样</p>
<h4 id="softmax解决方案"><a href="#softmax解决方案" class="headerlink" title="softmax解决方案"></a>softmax解决方案</h4><p><a href="https://hit-scir.gitbooks.io/neural-networks-and-deep-learning-zh_cn/content/chap3/c3s4.html" rel="external nofollow noopener noreferrer" target="_blank">略</a></p>
<h3 id="权值矩阵的初始化"><a href="#权值矩阵的初始化" class="headerlink" title="权值矩阵的初始化"></a>权值矩阵的初始化</h3><p>在梯度下降中，初始位置使用随机值，这个随机值服从标准正态分布<br><img src="http://bos.nj.bpc.baidu.com/v1/agroup/4e81bda549d68ba47b185714c2d25d4b0cbe0734" alt="图片"><br>现在考虑一个简化的情景，假设我们已经用标准正态分布初始化了第一层的权重值<br><img src="http://bos.nj.bpc.baidu.com/v1/agroup/a8a197341d15332a836f8d59c2966889dacc0336" alt="图片"><br>假设我们有1000个输入节点，500个输出1，500个输出0，这种情况下<br><img src="http://bos.nj.bpc.baidu.com/v1/agroup/4ca3f8788b1383457dca8243ae834641db31c125" alt="图片"><br>z受到501个参数的影响，z服从均值为0，标准差为sqrt(501)的正态分布</p>
<p><img src="http://bos.nj.bpc.baidu.com/v1/agroup/1cbad3be0cab9ae60288f0c444c188b7f1e7f90c" alt="图片"></p>
<p>这种情况和我们之前讨论过为什么最小二乘的代价函数配合sigmoid函数造成学习速度降低是一个道理。</p>
<p>如果我们对权值矩阵初始值做归一化操作，最终的z分布呈现<br><img src="http://bos.nj.bpc.baidu.com/v1/agroup/8fcdfb2dd61dfa593ec5ba352dbb74f40fcb2574" alt="图片"><br>这种情况，将尽可能减少学习速率降低的情况，在误差大的情况下，提高学习速率</p>
<p><img src="http://bos.nj.bpc.baidu.com/v1/agroup/28900731946af6a92c5a0e5160c75e241082f1a3" alt="图片"></p>
<p><a href="http://mp.weixin.qq.com/s?__biz=MzIxMjAzNDY5Mg==&amp;mid=2650790599&amp;idx=1&amp;sn=b28bf24f693aff48061ea4a2deb9bcab&amp;scene=21#wechat_redirect" rel="external nofollow noopener noreferrer" target="_blank">详细描述</a></p>
<h3 id="过拟合的抑制"><a href="#过拟合的抑制" class="headerlink" title="过拟合的抑制"></a>过拟合的抑制</h3><p>过拟合的问题是过度关注样本点在测量时的噪音，而忽略了真正隐藏在数据下面的通用性规律，因此需要对权值矩阵进行一定的限制。</p>
<h4 id="L2正则化"><a href="#L2正则化" class="headerlink" title="L2正则化"></a>L2正则化</h4><p>L2 的意思是平方，正则化也称为权重衰减，具体描述<br><img src="http://bos.nj.bpc.baidu.com/v1/agroup/62f63c1c18226e597e97acca2b710b1ffd77962f" alt="图片"><br>第一部分是交叉熵代价函数，第二部分是正则项，添加第二部分后对BP的影响<br><img src="http://bos.nj.bpc.baidu.com/v1/agroup/6f8535a8acbc88386202673b3c0852a531173e7a" alt="图片"></p>
<h2 id="为什么神经网络能够『解决』『所有』问题"><a href="#为什么神经网络能够『解决』『所有』问题" class="headerlink" title="为什么神经网络能够『解决』『所有』问题"></a>为什么神经网络能够『解决』『所有』问题</h2><p>神经网络能够解决什么样的问题？</p>
<p>答案是，能解决所有的问题。</p>
<h3 id="『解决』的含义"><a href="#『解决』的含义" class="headerlink" title="『解决』的含义"></a>『解决』的含义</h3><p>神经网络中对于解决的问题的定义是，能够以近似值的方式，尽量『趋近』真实的解决方案。这个解决方案是未知的，也不知道应该如何获取这个解决方案，但是，网络能够提供一种『最优解』，这个最优解能够提供接近真实解决方案的一个解决方案。这就是神经网络解决的含义</p>
<h3 id="『所有』的含义"><a href="#『所有』的含义" class="headerlink" title="『所有』的含义"></a>『所有』的含义</h3><p>首先，对问题规模进行一个精简，假设我们需要计算下面这个奇怪复杂的函数在某个x值点上y的值<br><img src="http://bos.nj.bpc.baidu.com/v1/agroup/8c808ffad2239dd86a6fcbe0bd9940e8de9d9e24" alt="图片"><br>面对这个函数曲线，我们既不知道这个函数背后的数学公式，也不知道问题的解决方案。但我们知道，不管函数什么样，总会有个输入x和输出y，我们不知道f(x)是什么，但是我们可以获得一个y’约等于f(x)。<br><img src="http://bos.nj.bpc.baidu.com/v1/agroup/e7eb8d14dd28070c157bb66446c1a99bc74907d8" alt="图片"><br>即时这个函数存在一个多维输入，通用可以将问题抽象为：<br><img src="http://bos.nj.bpc.baidu.com/v1/agroup/a1bbab233406da2157f9ec2514836d5ebf4f6cd8" alt="图片"></p>
<p>这意味着，神经网络具有一种普遍性，不论需要计算的是什么样的函数，我们都确定存在一个网络架构能够计算这个问题。<br>关键点在于，我们并没有真正获取函数对应的数学公式，我们也不知道真实的解是多少，但是我们可以获得一个近似值，并且这个近似值对应的误差是可控的。同时，我们的网络架构具备普适性，能够面对任意的函数。如果将函数推广到需要解决的问题：</p>
<ul>
<li>f(x) 代表将中文翻译为对应的英文</li>
<li>f(x) 代表将混合的多源语音进行分离</li>
<li>f(x) 代表将人脸图片映射为代表的人</li>
</ul>
<p>神经网络的这种普适性保证了我们能够解决『所有问题』。</p>
<p>可能存在的一个问题是，上述的普适性，面对的函数是连续的，如果函数是非连续的，类似阶跃函数这样的情景下，神经网络就不适应。</p>
<h3 id="如何解决"><a href="#如何解决" class="headerlink" title="如何解决"></a>如何解决</h3><p>前两节只是描述了神经网络能够对连续函数的输出值进行一个估计，并将误差控制在一个阈值内。那网络是如何解决这个问题？</p>
<p>我们先<strong><em>简化</em></strong>问题规模，解决的f(x)只有一维输入和一维输出，首先关注最上面的神经元。<br><img src="http://mmbiz.qpic.cn/mmbiz/58FUuNaBUjrcxwHE3MQxbaWKZgIr4VicavdQPVbFC5gAk6ClA1GkQJyEe9URv51geiaibQ5dDTgR7KgiaxsPw6afSw/0?wx_fmt=gif&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1" alt="图片"></p>
<ul>
<li>增加w的值，对应的输出将逐渐由sigmoid函数变化为阶跃函数</li>
<li>改变b的值，将平移输出结果</li>
</ul>
<blockquote>
<p>这里之所以要用阶跃函数取代sigmoid函数来分析问题，主要是一系列阶跃函数的累加值分析更加直观简单，相反，一连串s型的曲线叠加在一起是什么就更难分析</p>
</blockquote>
<p>关键点就在于阶跃函数发生阶跃的位置，这个位置可以由b控制，同时令s=-b/w，可以得到下述表现形式<br><img src="http://bos.nj.bpc.baidu.com/v1/agroup/5e47548e2052dc9de1fb5feda55a1b294eb10ff5" alt="图片"></p>
<p>我们控制w1的值，就可以控制阶跃的高度，两个阶跃叠加的结果</p>
<p><img src="http://bos.nj.bpc.baidu.com/v1/agroup/ceb016eca512dfa576f384b6988cf06cbdc856fa" alt="图片"></p>
<p>如果w1和w2的值相反，正负叠加的结果为一个单峰的阶跃值。<br><img src="http://bos.nj.bpc.baidu.com/v1/agroup/8473e532e2d1f8eb7a08c6c912d460a510ab92a7" alt="图片"></p>
<p>上述叠加发生在存在两个配对的神经元间，如果我们增加神经元的数量，则可以通过叠加获得更加丰富的结果。<br><img src="http://bos.nj.bpc.baidu.com/v1/agroup/6f34e2901e35670e82061623df6c82f679bc1b06" alt="图片"><br>现在回到最早的问题，我们要解决一个特殊的函数<br><img src="http://bos.nj.bpc.baidu.com/v1/agroup/2c1de2e0b8af6c515a3c131d77f9e71cb5108d9b" alt="图片"><br>这个函数实际上对应的函数是<br><img src="http://bos.nj.bpc.baidu.com/v1/agroup/1e5b091b6e9f6df72d8da0ed4fedef37f6501a4a" alt="图片"><br>虽然我们并不知道，但通过多个阶跃值进行模拟，就可以获得一个粗略的模拟值和一个与真实值自建的差异<br><img src="http://bos.nj.bpc.baidu.com/v1/agroup/f938a71c2fb21de7e377496f57fe2de51de7c631" alt="图片"><br>通过控制各个s与h的值，就能够保证在可以接受的误差阈值内获取一个模型，这个模型能够尽量接近真实结果，这就是神经网络能够训练所有问题的基础。</p>
<h2 id="后序"><a href="#后序" class="headerlink" title="后序"></a>后序</h2><ul>
<li><p>上述内容是在学习《神经网络与深度学习》之后的一些感悟，总结的有点杂乱，大概描述了看这本书过程中一些重要的理解。然后我决定以后看一本书，就写一篇读书笔记，花了一个月的时间，总不能就这样什么都没留下是吧。</p>
</li>
<li><p>看这本书，源自于某两人在某次开会时争论深度学习与机器学习是不是完全不同的两种技术。</p>
</li>
<li><p>以前还在糯米的时候，听晓大神说过，神经网络与深度学习就是把一堆输入提供给网络，我们不知道它是怎么训练的，也不知道各个中间值是什么意思，代表什么。这是深度学习和机器学习特别是监督学习最本质的区别。</p>
</li>
<li><p>看完nndl，要说能有什么马上能转化为产出的地方，其实并没有。但是，很多年前上编译原理的时候，大保健就说过，你花在读书上的时间永远不会浪费，哪怕这个东西你现在用不上，哪怕你过段时间都快忘了，总有一天，当你突然需要用时，泉思如涌，自然而然，厚积薄发。</p>
</li>
<li><p>看这本书的时候，我一直想起以前泡图书馆的日子，悠闲自在，有大把大把的时间可以去做自己想做的事。但是，那时候傻啊，屁都不懂就特么闷头学.NET，还看不懂英文书，白特么瞎了那么多时间。讲真，我适合搞学术。</p>
</li>
<li><p>然后，没然后了，上了年纪不比当年，反应有点慢。就这样吧，下本看的书是Bishop老爷子的在95年写的《Neural Networks for Pattern Recognition》，听闻比PRML简单。</p>
</li>
<li><p>提到PRML，我心里那个恨啊，特别是最近发现有个哈工大的大神，翻译了一整本书，92年的啊，92年的啊，我日了啊。</p>
</li>
<li><p>最后的最后，曾经有人看了文章，迫于我的气场，给我打赏了一分钱，没错，就特么一分钱，我祝他短1cm，张小哥，讲的就是你。</p>
</li>
</ul>

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        
  <div style="padding: 10px 0; margin: 20px auto; width: 90%; text-align: center;">
    <div>坚持原创技术分享，您的支持将鼓励我继续创作！</div>
    <button id="rewardButton" disable="enable" onclick="var qr = document.getElementById('QR'); if (qr.style.display === 'none') {qr.style.display='block';} else {qr.style.display='none'}">
      <span>赏</span>
    </button>
    <div id="QR" style="display: none;">
      
        <div id="wechat" style="display: inline-block">
          <img id="wechat_qr" src="/uploads/wechat-reward-image" alt="k神 WeChat Pay">
          <p>微信打赏</p>
        </div>
      
      
        <div id="alipay" style="display: inline-block">
          <img id="alipay_qr" src="/uploads/alipay-reward-image" alt="k神 Alipay">
          <p>支付宝打赏</p>
        </div>
      
    </div>
  </div>


      
    </div>


    <footer class="post-footer">
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2016/12/01/node-gc-part1/" rel="next" title="《Node.js学习笔记（一）：内存控制与垃圾回收》">
                <i class="fa fa-chevron-left"></i> 《Node.js学习笔记（一）：内存控制与垃圾回收》
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          

  <p>热评文章</p>
  <div class="ds-top-threads" data-range="weekly" data-num-items="4"></div>


          
  <div class="comments" id="comments">
    
      <div class="ds-thread" data-thread-key="2017/05/07/《神经网络与深度学习》学习笔记/" data-title="《神经网络与深度学习》学习笔记" data-url="http://yoursite.com/2017/05/07/《神经网络与深度学习》学习笔记/">
      </div>
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image" src="/uploads/avatar.png" alt="k神">
          <p class="site-author-name" itemprop="name">k神</p>
          <p class="site-description motion-element" itemprop="description">做一名纯粹中庸的小白</p>
        </div>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">3</span>
              <span class="site-state-item-name">日志</span>
            </a>
          </div>

          

          
            <div class="site-state-item site-state-tags">
              <a href="/tags">
                <span class="site-state-item-count">1</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        
          <div class="feed-link motion-element">
            <a href="/atom.xml" rel="alternate">
              <i class="fa fa-rss"></i>
              RSS
            </a>
          </div>
        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/proxukun" target="_blank" title="GitHub" rel="external nofollow noopener noreferrer">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                  GitHub
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://www.douban.com/people/85375509/" target="_blank" title="豆瓣" rel="external nofollow noopener noreferrer">
                  
                    <i class="fa fa-fw fa-globe"></i>
                  
                  豆瓣
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://www.zhihu.com/people/proxukun/activities" target="_blank" title="知乎" rel="external nofollow noopener noreferrer">
                  
                    <i class="fa fa-fw fa-globe"></i>
                  
                  知乎
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="http://t.qq.com/proxukun" target="_blank" title="微博" rel="external nofollow noopener noreferrer">
                  
                    <i class="fa fa-fw fa-globe"></i>
                  
                  微博
                </a>
              </span>
            
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#目录"><span class="nav-number">1.</span> <span class="nav-text">目录</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#神经网络进行学习的基本前提"><span class="nav-number">2.</span> <span class="nav-text">神经网络进行学习的基本前提</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#神经网络两个假设与四个等式"><span class="nav-number">3.</span> <span class="nav-text">神经网络两个假设与四个等式</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#代价函数的两个假设"><span class="nav-number">3.1.</span> <span class="nav-text">代价函数的两个假设</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#假设1"><span class="nav-number">3.1.1.</span> <span class="nav-text">假设1</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#假设2"><span class="nav-number">3.1.2.</span> <span class="nav-text">假设2</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#BP背后的四个基本等式"><span class="nav-number">3.2.</span> <span class="nav-text">BP背后的四个基本等式</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#BP1"><span class="nav-number">3.2.1.</span> <span class="nav-text">BP1</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#BP2"><span class="nav-number">3.2.2.</span> <span class="nav-text">BP2</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#BP3"><span class="nav-number">3.2.3.</span> <span class="nav-text">BP3</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#BP4"><span class="nav-number">3.2.4.</span> <span class="nav-text">BP4</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#实现"><span class="nav-number">4.</span> <span class="nav-text">实现</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#超参数的选择"><span class="nav-number">5.</span> <span class="nav-text">超参数的选择</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#存在的问题与优化点"><span class="nav-number">6.</span> <span class="nav-text">存在的问题与优化点</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#学习速率的优化"><span class="nav-number">6.1.</span> <span class="nav-text">学习速率的优化</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#学习速率慢的真正原因"><span class="nav-number">6.1.1.</span> <span class="nav-text">学习速率慢的真正原因</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#交叉熵解决方案"><span class="nav-number">6.1.2.</span> <span class="nav-text">交叉熵解决方案</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#关于代价函数的修改对算法的影响"><span class="nav-number">6.1.2.1.</span> <span class="nav-text">关于代价函数的修改对算法的影响</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#softmax解决方案"><span class="nav-number">6.1.3.</span> <span class="nav-text">softmax解决方案</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#权值矩阵的初始化"><span class="nav-number">6.2.</span> <span class="nav-text">权值矩阵的初始化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#过拟合的抑制"><span class="nav-number">6.3.</span> <span class="nav-text">过拟合的抑制</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#L2正则化"><span class="nav-number">6.3.1.</span> <span class="nav-text">L2正则化</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#为什么神经网络能够『解决』『所有』问题"><span class="nav-number">7.</span> <span class="nav-text">为什么神经网络能够『解决』『所有』问题</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#『解决』的含义"><span class="nav-number">7.1.</span> <span class="nav-text">『解决』的含义</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#『所有』的含义"><span class="nav-number">7.2.</span> <span class="nav-text">『所有』的含义</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#如何解决"><span class="nav-number">7.3.</span> <span class="nav-text">如何解决</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#后序"><span class="nav-number">8.</span> <span class="nav-text">后序</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js">
</script>
<div class="copyright">
  
  &copy;  2015 - 
  <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">k神</span>

</div>


        

        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  



  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.0"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.0"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script>



  

  
    
  

  <script type="text/javascript">
    var duoshuoQuery = {short_name:"proxukun"};
    (function() {
      var ds = document.createElement('script');
      ds.type = 'text/javascript';ds.async = true;
      ds.id = 'duoshuo-script';
      ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
      ds.charset = 'UTF-8';
      (document.getElementsByTagName('head')[0]
      || document.getElementsByTagName('body')[0]).appendChild(ds);
    })();
  </script>

  
    
    
    <script src="/lib/ua-parser-js/dist/ua-parser.min.js?v=0.7.9"></script>
    <script src="/js/src/hook-duoshuo.js"></script>
  








  
  
  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length == 0) {
      search_path = "search.xml";
    }
    var path = "/" + search_path;
    // monitor main search box;

    function proceedsearch() {
      $("body").append('<div class="popoverlay">').css('overflow', 'hidden');
      $('.popup').toggle();
    }
    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';
      $.ajax({
        url: path,
        dataType: "xml",
        async: true,
        success: function( xmlResponse ) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = $( "entry", xmlResponse ).map(function() {
            return {
              title: $( "title", this ).text(),
              content: $("content",this).text(),
              url: $( "url" , this).text()
            };
          }).get();
          var $input = document.getElementById(search_id);
          var $resultContent = document.getElementById(content_id);
          $input.addEventListener('input', function(){
            var matchcounts = 0;
            var str='<ul class=\"search-result-list\">';
            var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
            $resultContent.innerHTML = "";
            if (this.value.trim().length > 1) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var content_index = [];
                var data_title = data.title.trim().toLowerCase();
                var data_content = data.content.trim().replace(/<[^>]+>/g,"").toLowerCase();
                var data_url = decodeURIComponent(data.url);
                var index_title = -1;
                var index_content = -1;
                var first_occur = -1;
                // only match artiles with not empty titles and contents
                if(data_title != '') {
                  keywords.forEach(function(keyword, i) {
                    index_title = data_title.indexOf(keyword);
                    index_content = data_content.indexOf(keyword);
                    if( index_title >= 0 || index_content >= 0 ){
                      isMatch = true;
                      if (i == 0) {
                        first_occur = index_content;
                      }
                    }

                  });
                }
                // show search results
                if (isMatch) {
                  matchcounts += 1;
                  str += "<li><a href='"+ data_url +"' class='search-result-title'>"+ data_title +"</a>";
                  var content = data.content.trim().replace(/<[^>]+>/g,"");
                  if (first_occur >= 0) {
                    // cut out 100 characters
                    var start = first_occur - 20;
                    var end = first_occur + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if(start == 0){
                      end = 50;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    var match_content = content.substring(start, end);
                    // highlight all keywords
                    keywords.forEach(function(keyword){
                      var regS = new RegExp(keyword, "gi");
                      match_content = match_content.replace(regS, "<b class=\"search-keyword\">"+keyword+"</b>");
                    });

                    str += "<p class=\"search-result\">" + match_content +"...</p>"
                  }
                  str += "</li>";
                }
              })};
            str += "</ul>";
            if (matchcounts == 0) { str = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>' }
            if (keywords == "") { str = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>' }
            $resultContent.innerHTML = str;
          });
          proceedsearch();
        }
      });}

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched == false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(function(e){
      $('.popup').hide();
      $(".popoverlay").remove();
      $('body').css('overflow', '');
    });
    $('.popup').click(function(e){
      e.stopPropagation();
    });
  </script>


  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.1.js"></script>
  <script>AV.initialize("9gx431jY99kp4LtAGkaorKmO-gzGzoHsz", "KGlUxue9je1nwznXa4xId6pz");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
          for(var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = document.getElementById(url);
            var countSpan = $(element).find(COUNT_CONTAINER_REF);
            if( countSpan.text() == '') {
              countSpan.text(0);
            }
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            /* Set ACL */
            var acl = new AV.ACL();
            acl.setPublicReadAccess(true);
            acl.setPublicWriteAccess(true);
            newcounter.setACL(acl);
            /* End Set ACL */
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>



  

  


</body>
</html>
