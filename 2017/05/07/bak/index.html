<!doctype html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">



<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">












  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css">


  <meta name="keywords" content="Hexo, NexT">




  


  <link rel="alternate" href="/atom.xml" title="proxukun" type="application/atom+xml">




  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.0">






<meta name="description" content="康肃问曰：”汝亦知射乎？吾射不亦精乎？”翁曰：”无他， 但手熟尔。”

目录
神经网络进行学习的基本前提  
神经网络两个假设与四个等式
实现  
超参数的选择   
存在的问题与优化点   
为什么神经网络能够『解决』『所有』问题

神经网络进行学习的基本前提神经网络对输入样本点的学习，主要是根据不断修改权值与偏移值，保证输出结果朝着可预期的方向不断改善。为了了解整个过程，我们首先观察权值矩阵和">
<meta property="og:type" content="article">
<meta property="og:title" content="《神经网络与深度学习》学习笔记">
<meta property="og:url" content="http://yoursite.com/2017/05/07/bak/index.html">
<meta property="og:site_name" content="proxukun">
<meta property="og:description" content="康肃问曰：”汝亦知射乎？吾射不亦精乎？”翁曰：”无他， 但手熟尔。”

目录
神经网络进行学习的基本前提  
神经网络两个假设与四个等式
实现  
超参数的选择   
存在的问题与优化点   
为什么神经网络能够『解决』『所有』问题

神经网络进行学习的基本前提神经网络对输入样本点的学习，主要是根据不断修改权值与偏移值，保证输出结果朝着可预期的方向不断改善。为了了解整个过程，我们首先观察权值矩阵和">
<meta property="og:image" content="http://bos.nj.bpc.baidu.com/v1/agroup/c27adf4d2967b7abc46b888637d38cd9b14cb1e6">
<meta property="og:image" content="http://bos.nj.bpc.baidu.com/v1/agroup/2732cac8ed3188e69b0e58017a55f80534833b3f">
<meta property="og:image" content="http://bos.nj.bpc.baidu.com/v1/agroup/cc5e412675ecc1f2d067434a25da6b7dac692f97">
<meta property="og:image" content="http://bos.nj.bpc.baidu.com/v1/agroup/db79325ca40ef3dd364c432abb0bebb0ff532207">
<meta property="og:image" content="http://bos.nj.bpc.baidu.com/v1/agroup/66601d09675d412db941f059cffaf677a5cfbad0">
<meta property="og:image" content="http://bos.nj.bpc.baidu.com/v1/agroup/b04a43288772e799a5d58bff376f85298986071a">
<meta property="og:image" content="http://bos.nj.bpc.baidu.com/v1/agroup/81242ff69b3e5a787a56a0363a69aff50b045484">
<meta property="og:image" content="http://bos.nj.bpc.baidu.com/v1/agroup/d441e4afa6758a7329621a90413ffd7fc9fdeada">
<meta property="og:image" content="http://bos.nj.bpc.baidu.com/v1/agroup/c2962ff2362a7a9e0aadf095545e99ed00310837">
<meta property="og:image" content="http://bos.nj.bpc.baidu.com/v1/agroup/9443b29af17200ad396b44ef29ff82578048f7cc">
<meta property="og:image" content="http://bos.nj.bpc.baidu.com/v1/agroup/e6752f940062f1d87e7c4f80837c4dc393a4b024">
<meta property="og:image" content="http://bos.nj.bpc.baidu.com/v1/agroup/53791aa683817780aa4155d2f699ed9c3ea8cc3f">
<meta property="og:image" content="http://bos.nj.bpc.baidu.com/v1/agroup/3267fbde7976bd97cd434c8bf3206a6b1e442ba8">
<meta property="og:image" content="http://bos.nj.bpc.baidu.com/v1/agroup/195549767f60fa780284ad58f6fd6a56bb307a17">
<meta property="og:image" content="http://bos.nj.bpc.baidu.com/v1/agroup/2f0840b3ccffe18057c9621f118deb6d722cc516">
<meta property="og:image" content="http://bos.nj.bpc.baidu.com/v1/agroup/db79325ca40ef3dd364c432abb0bebb0ff532207">
<meta property="og:image" content="http://bos.nj.bpc.baidu.com/v1/agroup/bbf991c36852ffc444196228d51d618e7d6d87c2">
<meta property="og:image" content="http://bos.nj.bpc.baidu.com/v1/agroup/21db3e1aa4287877c5d73fdd54c0fc4e65aa2b85">
<meta property="og:image" content="http://bos.nj.bpc.baidu.com/v1/agroup/2c0b4e278e37356fca9a3b70d0f797a1511c9eb2">
<meta property="og:image" content="http://bos.nj.bpc.baidu.com/v1/agroup/1b5e331b60347a32d782d3d480e1da182adece01">
<meta property="og:image" content="http://bos.nj.bpc.baidu.com/v1/agroup/25acb007888a3ca38cd937b3f2f94b92c2c6c1df">
<meta property="og:image" content="http://bos.nj.bpc.baidu.com/v1/agroup/fb3436c0161c58556687a1749bc832b4adbbba75">
<meta property="og:image" content="http://bos.nj.bpc.baidu.com/v1/agroup/e6291b0709f95e6d0d1b83a3bf7f52d51b01eb10">
<meta property="og:image" content="http://bos.nj.bpc.baidu.com/v1/agroup/8c808ffad2239dd86a6fcbe0bd9940e8de9d9e24">
<meta property="og:image" content="http://bos.nj.bpc.baidu.com/v1/agroup/e7eb8d14dd28070c157bb66446c1a99bc74907d8">
<meta property="og:image" content="http://bos.nj.bpc.baidu.com/v1/agroup/a1bbab233406da2157f9ec2514836d5ebf4f6cd8">
<meta property="og:image" content="http://mmbiz.qpic.cn/mmbiz/58FUuNaBUjrcxwHE3MQxbaWKZgIr4VicavdQPVbFC5gAk6ClA1GkQJyEe9URv51geiaibQ5dDTgR7KgiaxsPw6afSw/0?wx_fmt=gif&tp=webp&wxfrom=5&wx_lazy=1">
<meta property="og:image" content="http://bos.nj.bpc.baidu.com/v1/agroup/5e47548e2052dc9de1fb5feda55a1b294eb10ff5">
<meta property="og:image" content="http://bos.nj.bpc.baidu.com/v1/agroup/ceb016eca512dfa576f384b6988cf06cbdc856fa">
<meta property="og:image" content="http://bos.nj.bpc.baidu.com/v1/agroup/8473e532e2d1f8eb7a08c6c912d460a510ab92a7">
<meta property="og:image" content="http://bos.nj.bpc.baidu.com/v1/agroup/6f34e2901e35670e82061623df6c82f679bc1b06">
<meta property="og:image" content="http://bos.nj.bpc.baidu.com/v1/agroup/2c1de2e0b8af6c515a3c131d77f9e71cb5108d9b">
<meta property="og:image" content="http://bos.nj.bpc.baidu.com/v1/agroup/1e5b091b6e9f6df72d8da0ed4fedef37f6501a4a">
<meta property="og:image" content="http://bos.nj.bpc.baidu.com/v1/agroup/f938a71c2fb21de7e377496f57fe2de51de7c631">
<meta property="og:updated_time" content="2017-05-07T13:10:16.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="《神经网络与深度学习》学习笔记">
<meta name="twitter:description" content="康肃问曰：”汝亦知射乎？吾射不亦精乎？”翁曰：”无他， 但手熟尔。”

目录
神经网络进行学习的基本前提  
神经网络两个假设与四个等式
实现  
超参数的选择   
存在的问题与优化点   
为什么神经网络能够『解决』『所有』问题

神经网络进行学习的基本前提神经网络对输入样本点的学习，主要是根据不断修改权值与偏移值，保证输出结果朝着可预期的方向不断改善。为了了解整个过程，我们首先观察权值矩阵和">
<meta name="twitter:image" content="http://bos.nj.bpc.baidu.com/v1/agroup/c27adf4d2967b7abc46b888637d38cd9b14cb1e6">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    sidebar: {"position":"left","display":"post"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '6356875390790665000',
      author: '主编大人'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2017/05/07/bak/">





  <title> 《神经网络与深度学习》学习笔记 | proxukun </title>
</head>

<body itemscope="" itemtype="http://schema.org/WebPage" lang="zh-Hans">

  



  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "//hm.baidu.com/hm.js?a0a4930e55e55530ff0bab49e40d2f6e";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>








  
  
    
  

  <div class="container one-collumn sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">proxukun</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
  <p class="site-subtitle"></p>
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br>
            
            关于
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup">
 <span class="search-icon fa fa-search"></span>
 <input type="text" id="local-search-input">
 <div id="local-search-result"></div>
 <span class="popup-btn-close">close</span>
</div>


    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal " itemscope="" itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/05/07/bak/">

  <span style="display:none" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
    <meta itemprop="name" content="k神">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/uploads/avatar.png">
  </span>

  <span style="display:none" itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="proxukun">
    <span style="display:none" itemprop="logo" itemscope="" itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="proxukun" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                《神经网络与深度学习》学习笔记
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-05-07T21:08:34+08:00">
                2017-05-07
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <a href="/2017/05/07/bak/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2017/05/07/bak/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          

          
          
             <span id="/2017/05/07/bak/" class="leancloud_visitors" data-flag-title="《神经网络与深度学习》学习笔记">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               <span class="post-meta-item-text">阅读次数 </span>
               <span class="leancloud-visitors-count"></span>
              </span>
          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        <blockquote class="blockquote-center"><br><footer style="textalign:center;"><br>康肃问曰：”汝亦知射乎？吾射不亦精乎？”翁曰：”无他， 但手熟尔。”<br></footer></blockquote>

<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul>
<li>神经网络进行学习的基本前提  </li>
<li>神经网络两个假设与四个等式</li>
<li>实现  </li>
<li>超参数的选择   </li>
<li>存在的问题与优化点   </li>
<li>为什么神经网络能够『解决』『所有』问题</li>
</ul>
<h2 id="神经网络进行学习的基本前提"><a href="#神经网络进行学习的基本前提" class="headerlink" title="神经网络进行学习的基本前提"></a>神经网络进行学习的基本前提</h2><p>神经网络对输入样本点的学习，主要是根据不断修改权值与偏移值，保证输出结果朝着可预期的方向不断改善。为了了解整个过程，我们首先观察权值矩阵和偏移值矩阵的变化如何影响最终结果的输出。当我们对权值矩阵进行了一个微小的修改：<br><img src="http://bos.nj.bpc.baidu.com/v1/agroup/c27adf4d2967b7abc46b888637d38cd9b14cb1e6" alt="图片"><br>　<br><a id="more"></a>　<br>这个修改首先会反应在该神经元的激活值上：<br><img src="http://bos.nj.bpc.baidu.com/v1/agroup/2732cac8ed3188e69b0e58017a55f80534833b3f" alt="图片"></p>
<p>这个激活值的变化会影响所有下一层邻接神经元的输出值，并最终反应到输出层的变化：<br>　　<br><img src="http://bos.nj.bpc.baidu.com/v1/agroup/cc5e412675ecc1f2d067434a25da6b7dac692f97" alt="图片"><br>　　<br>如果选中其中一条变化的路线，可以用公式描述整个过程：<br><img src="http://bos.nj.bpc.baidu.com/v1/agroup/db79325ca40ef3dd364c432abb0bebb0ff532207" alt="图片"></p>
<p>如果将中间多个隐藏层与不同路线的变化当做一个黑盒，可以用公式来描述整个过程：<br><img src="http://bos.nj.bpc.baidu.com/v1/agroup/66601d09675d412db941f059cffaf677a5cfbad0" alt="图片"></p>
<p>其中，<img src="http://bos.nj.bpc.baidu.com/v1/agroup/b04a43288772e799a5d58bff376f85298986071a" alt="图片">描述的是最终结果对wjk的敏感程度，该公式也可以根据链式法则展开获得完整的变化。<br>　　<br>上述变化过程，在整个神经网络训练过程中不断重复，通过对权值矩阵和偏倚值矩阵不断『轻微』的『试探』，刺激输出值进行一个『轻微』的反馈，根据反馈的优劣改变刺激方向与程度，寻找一个相对可行的优化『路线』，并最终到达一个『最优值』。重复这个过程，就能保证整个网络在数据的基础上正常的运作起来。如下图所示：<br><img src="http://bos.nj.bpc.baidu.com/v1/agroup/81242ff69b3e5a787a56a0363a69aff50b045484" alt="图片"></p>
<p>因此，问题的关键在于保证『轻微』的变化会给予输出结果『轻微』的变化，如果可以，我们希望这种变化『朴素』到仅仅与变化源头的神经元存在关联。<br><img src="http://bos.nj.bpc.baidu.com/v1/agroup/d441e4afa6758a7329621a90413ffd7fc9fdeada" alt="图片"><br>如上述公式所描述，最终结果的变化为权值与偏倚值值的线性变化，这样，我们就可以控制Delta值的系数，以此保证了输出值变化的可控程度。</p>
<p>在此基础上，我们考虑，如何来衡量『轻微』。<br><img src="http://bos.nj.bpc.baidu.com/v1/agroup/c2962ff2362a7a9e0aadf095545e99ed00310837" alt="图片"><br>『轻微』的含义是，变化源头的神经源微小的变化，不论中间隐藏层各个神经元如何变化，对输出也造成相同程度的变化。因此我们需要保证激活值的变化是先对『连续』的。<br><img src="http://bos.nj.bpc.baidu.com/v1/agroup/9443b29af17200ad396b44ef29ff82578048f7cc" alt="图片"><br>如果选择阶跃函数作为激活函数，这一个输入的轻微变化可能导致输出由0变成1，进而影响下一层所有邻接神经元，最终对输出造成极大的变化。<br><img src="http://bos.nj.bpc.baidu.com/v1/agroup/e6752f940062f1d87e7c4f80837c4dc393a4b024" alt="图片"><br>因而，在神经网络中，经常选择sigmoid function作为激活函数，这类神经元也被称为S型神经元。权值与偏倚值值的微小变化，同样反应在输出值的微小变化。</p>
<h2 id="神经网络两个假设与四个等式"><a href="#神经网络两个假设与四个等式" class="headerlink" title="神经网络两个假设与四个等式"></a>神经网络两个假设与四个等式</h2><h3 id="代价函数的两个假设"><a href="#代价函数的两个假设" class="headerlink" title="代价函数的两个假设"></a>代价函数的两个假设</h3><p>最简单的代价函数是用来衡量输出结果与真实结果间的一种误差程度，进而用来知道权值矩阵和偏倚值矩阵的优化方向。</p>
<p>平方代价函数：<br><img src="http://bos.nj.bpc.baidu.com/v1/agroup/53791aa683817780aa4155d2f699ed9c3ea8cc3f" alt="图片"></p>
<p>交叉熵代价函数：<br><img src="http://bos.nj.bpc.baidu.com/v1/agroup/3267fbde7976bd97cd434c8bf3206a6b1e442ba8" alt="图片"></p>
<p>关于代价函数的选择，有两个基本的假设作为前提。</p>
<h4 id="假设1"><a href="#假设1" class="headerlink" title="假设1"></a>假设1</h4><p><img src="http://bos.nj.bpc.baidu.com/v1/agroup/195549767f60fa780284ad58f6fd6a56bb307a17" alt="图片"><br>代价函数能够被写成上述表达形式，其中C为整体代价，Cx为单个训练样本点的代价。该假设存在的原因是在训练过程中，都是以单个样本点作为单位进行训练。</p>
<h4 id="假设2"><a href="#假设2" class="headerlink" title="假设2"></a>假设2</h4><p>第二条假设是代价函数能够被写成最终激活值的函数表示，如下所示：<br><img src="http://bos.nj.bpc.baidu.com/v1/agroup/2f0840b3ccffe18057c9621f118deb6d722cc516" alt="图片"><br>该假设存在的原因是只有激活值是直接与权值矩阵，偏倚值矩阵关联在一起，因此只有将代价函数与激活值关联在一起，才能对这两者优化方向进行判定。</p>
<h3 id="BP背后的四个基本等式"><a href="#BP背后的四个基本等式" class="headerlink" title="BP背后的四个基本等式"></a>BP背后的四个基本等式</h3><p>基于梯度下降算法的神经网络训练在上世纪八十年代就已经被提出，但是，优于weight与biases的梯度计算，需要考虑所有可能的路线，每条路线需要根据链式法则计算各个偏导值，这些偏导值存在大量的重复计算，当隐藏层数量与神经元数量增加，计算量将爆炸增长。可行的解决方案包括使用DP记录各个偏导的计算结果，以空间换时间的方式来降低计算时间，不过不知道为什么DP并没有应用在神经网络的训练中。<br><img src="http://bos.nj.bpc.baidu.com/v1/agroup/db79325ca40ef3dd364c432abb0bebb0ff532207" alt="图片"></p>
<p>后来，有大神提出了通过后向反馈将误差值逐层『平摊』到各个神经元上，而平摊考虑到某个神经元对下一层邻接神经元的『贡献量』，而『贡献量』关联了权值矩阵和偏倚值矩阵的梯度，这个梯度又被应用与梯度下降时目标的<strong><em>学习速率</em></strong>，整个过程只需要从后向前传播一次即可。这种方法极大降低了神经网络训练过程中的计算量，也标志这神经网络训练的实用化。</p>
<blockquote>
<p>提到学习速率这个指标，有些资料会说η就是学习速率。 η真正代表的是学习步长。在梯度下降训练中，η（步长）表示每次沿梯度最大方向下降一步的距离。真正的学习速率，应该是权值矩阵与偏倚值矩阵的梯度大小，梯度越大，代表学习速率越快。</p>
</blockquote>
<p>整个后向反馈神经网络的训练，就是将一个误差变量与各个偏导关联在一起，通过这个中间变量来完成偏导的获取，进而实现更新。这两者的关系，可以浓缩为四个基本等式：</p>
<p><img src="http://bos.nj.bpc.baidu.com/v1/agroup/bbf991c36852ffc444196228d51d618e7d6d87c2" alt="图片"></p>
<h4 id="BP1"><a href="#BP1" class="headerlink" title="BP1"></a>BP1</h4><p>方程一描述的是误差变量的定义，这个变量存在的目的是将反向传播过程中权值矩阵和偏倚值值矩阵的梯度联系起来。<br><img src="http://bos.nj.bpc.baidu.com/v1/agroup/21db3e1aa4287877c5d73fdd54c0fc4e65aa2b85" alt="图片"></p>
<h4 id="BP2"><a href="#BP2" class="headerlink" title="BP2"></a>BP2</h4><p>方程二描述的是反向传播过程中误差值的传播，从输出层的误差向前传播，并将误差逐层记录，推导过程如下：<br><img src="http://bos.nj.bpc.baidu.com/v1/agroup/2c0b4e278e37356fca9a3b70d0f797a1511c9eb2" alt="图片"></p>
<h4 id="BP3"><a href="#BP3" class="headerlink" title="BP3"></a>BP3</h4><p>方程三将误差变量与偏倚值矩阵的梯度关联在一起<br><img src="http://bos.nj.bpc.baidu.com/v1/agroup/1b5e331b60347a32d782d3d480e1da182adece01" alt="图片"></p>
<h4 id="BP4"><a href="#BP4" class="headerlink" title="BP4"></a>BP4</h4><p>方程四将误差变量与权重矩阵的梯度关联在一起<br><img src="http://bos.nj.bpc.baidu.com/v1/agroup/25acb007888a3ca38cd937b3f2f94b92c2c6c1df" alt="图片"></p>
<h2 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h2><p>两个假设和BP1-4指定了在神经网络训练过程中权值矩阵和偏倚值矩阵梯度的获取方式，通过类似DP的方式降低了求导的计算量。在原有的计算方法中，对weight或biases的偏导通过链式法则的方式计算，存在极大的重复计算量，特别是在节点规模和weight矩阵规模增大时将呈指数增长。</p>
<p><img src="http://bos.nj.bpc.baidu.com/v1/agroup/fb3436c0161c58556687a1749bc832b4adbbba75" alt="图片"><br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">SGD</span><span class="params">(self, train_data, epoll, mini_batch_size, eta, test_data = None)</span>:</span></div><div class="line">    <span class="keyword">if</span> test_data: n_test = len(test_data)</div><div class="line">    n = len(train_data)</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> xrange(epoll):</div><div class="line">        random.shuffle(train_data)</div><div class="line">        mini_batches = [train_data[j:j+mini_batch_size] <span class="keyword">for</span> j <span class="keyword">in</span> xrange(<span class="number">0</span>, n, mini_batch_size)]</div><div class="line">        <span class="keyword">for</span> mini_batch <span class="keyword">in</span> mini_batches:</div><div class="line">           self.update_mini_batch(mini_batch, eta)</div><div class="line"></div><div class="line">        <span class="keyword">if</span> test_data:</div><div class="line">            <span class="keyword">print</span> <span class="string">"Epoll &#123;0&#125;: &#123;1&#125; / &#123;2&#125;"</span>.format(i,self.evaluate(test_data),n_test)</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            <span class="keyword">print</span> <span class="string">"Epoll &#123;0&#125; complete"</span>.format(i)</div></pre></td></tr></table></figure></p>
<p>SGD是入口函数，主要完成训练样本的散列和分批计算，通过调用update_mini_batch，完成批数据的训练</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">backpop</span><span class="params">(self, x, y)</span>:</span></div><div class="line">    nndl_w = [np.zeros(np.shape(item)) <span class="keyword">for</span> item <span class="keyword">in</span> self.weights]</div><div class="line">    nndl_b = [np.zeros(np.shape(item)) <span class="keyword">for</span> item <span class="keyword">in</span> self.biases]</div><div class="line"></div><div class="line">    z, zs, activations = self.feedforword(x)</div><div class="line"></div><div class="line">    <span class="comment">#delta = self.cost_derivative(activations[-1], y) * sigmoid_prime(z)</span></div><div class="line">    delta = self.cost_derivative(activations[<span class="number">-1</span>], y)</div><div class="line">    nndl_b[<span class="number">-1</span>] = delta</div><div class="line">    nndl_w[<span class="number">-1</span>] = np.dot(delta, activations[<span class="number">-2</span>].transpose())</div><div class="line"></div><div class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> xrange(<span class="number">2</span>, self.num_layers):</div><div class="line">        delta = np.dot(self.weights[-l+<span class="number">1</span>].transpose(),delta) * sigmoid_prime(zs[-l])</div><div class="line">        nndl_b[-l] = delta</div><div class="line">        nndl_w[-l] = np.dot(delta, activations[-l<span class="number">-1</span>].transpose())</div><div class="line"></div><div class="line">    <span class="keyword">return</span> nndl_b, nndl_w</div></pre></td></tr></table></figure>
<p>backpop是在反向传播过程中，逐层计算误差变量的值，并根据BP3与BP4更新权值矩阵和偏倚值矩阵的梯度。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_mini_batch</span><span class="params">(self, mini_batch, eta)</span>:</span></div><div class="line">    nndl_w = [np.zeros(item.shape) <span class="keyword">for</span> item <span class="keyword">in</span> self.weights]</div><div class="line">    nndl_b = [np.zeros(item.shape) <span class="keyword">for</span> item <span class="keyword">in</span> self.biases]</div><div class="line"></div><div class="line">    <span class="keyword">for</span> x,y <span class="keyword">in</span> mini_batch:</div><div class="line">        b, w = self.backpop(x, y)</div><div class="line">        nndl_b = [b + nb <span class="keyword">for</span> b, nb <span class="keyword">in</span> zip(nndl_b, b)]</div><div class="line">        nndl_w = [w + nw <span class="keyword">for</span> w, nw <span class="keyword">in</span> zip(nndl_w, w)]</div><div class="line"></div><div class="line">    self.biases= [b - (eta/len(mini_batch))*nb <span class="keyword">for</span> b, nb <span class="keyword">in</span> zip(self.biases, nndl_b)]</div><div class="line">    self.weights = [w - (eta/len(mini_batch))*nw <span class="keyword">for</span> w, nw <span class="keyword">in</span> zip(self.weights, nndl_w)]</div></pre></td></tr></table></figure>
<p>update_mini_batch主要完成样本点的前向传播，后向反馈过程，并在后向反馈过程中更新权值矩阵和便宜矩阵。<br>其中：</p>
<ul>
<li>backpop完成后向反馈过程，并将误差变量记录 </li>
<li>weight和biases的更新使用梯度下降方式更新</li>
</ul>
<p><img src="http://bos.nj.bpc.baidu.com/v1/agroup/e6291b0709f95e6d0d1b83a3bf7f52d51b01eb10" alt="图片"></p>
<h2 id="超参数的选择"><a href="#超参数的选择" class="headerlink" title="超参数的选择"></a>超参数的选择</h2><h3 id="mini-batch-size"><a href="#mini-batch-size" class="headerlink" title="mini_batch_size"></a>mini_batch_size</h3><h3 id="epoll"><a href="#epoll" class="headerlink" title="epoll"></a>epoll</h3><h3 id="lamda"><a href="#lamda" class="headerlink" title="lamda"></a>lamda</h3><h3 id="η"><a href="#η" class="headerlink" title="η"></a>η</h3><h2 id="存在的问题与优化点"><a href="#存在的问题与优化点" class="headerlink" title="存在的问题与优化点"></a>存在的问题与优化点</h2><p>网络训练过程中，最终要的有两点，训练时的学习速率和过拟合的抑制，前者保证网络能够更快的获取结果，特别是在训练初期，对超超速调整时；后者主要衡量训练出来的模型是否很好的抽象出一般性的通用规则。</p>
<h3 id="学习速率的优化"><a href="#学习速率的优化" class="headerlink" title="学习速率的优化"></a>学习速率的优化</h3><h4 id="学习速率慢的真正原因"><a href="#学习速率慢的真正原因" class="headerlink" title="学习速率慢的真正原因"></a>学习速率慢的真正原因</h4><h4 id="交叉熵解决方案"><a href="#交叉熵解决方案" class="headerlink" title="交叉熵解决方案"></a>交叉熵解决方案</h4><h3 id="过拟合的抑制"><a href="#过拟合的抑制" class="headerlink" title="过拟合的抑制"></a>过拟合的抑制</h3><h2 id="为什么神经网络能够『解决』『所有』问题"><a href="#为什么神经网络能够『解决』『所有』问题" class="headerlink" title="为什么神经网络能够『解决』『所有』问题"></a>为什么神经网络能够『解决』『所有』问题</h2><p>神经网络能够解决什么样的问题？</p>
<p>答案是，能解决所有的问题。</p>
<h3 id="『解决』的含义"><a href="#『解决』的含义" class="headerlink" title="『解决』的含义"></a>『解决』的含义</h3><p>神经网络中对于解决的问题的定义是，能够以近似值的方式，尽量『趋近』真实的解决方案。这个解决方案是未知的，也不知道应该如何获取这个解决方案，但是，网络能够提供一种『最优解』，这个最优解能够提供接近真实解决方案的一个解决方案。这就是神经网络解决的含义</p>
<h3 id="『所有』的含义"><a href="#『所有』的含义" class="headerlink" title="『所有』的含义"></a>『所有』的含义</h3><p>首先，对问题规模进行一个精简，假设我们需要计算下面这个奇怪复杂的函数在某个x值点上y的值<br><img src="http://bos.nj.bpc.baidu.com/v1/agroup/8c808ffad2239dd86a6fcbe0bd9940e8de9d9e24" alt="图片"><br>面对这个函数曲线，我们既不知道这个函数背后的数学公式，也不知道问题的解决方案。但我们知道，不管函数什么样，总会有个输入x和输出y，我们不知道f(x)是什么，但是我们可以获得一个y’约等于f(x)。<br><img src="http://bos.nj.bpc.baidu.com/v1/agroup/e7eb8d14dd28070c157bb66446c1a99bc74907d8" alt="图片"><br>即时这个函数存在一个多维输入，通用可以将问题抽象为：<br><img src="http://bos.nj.bpc.baidu.com/v1/agroup/a1bbab233406da2157f9ec2514836d5ebf4f6cd8" alt="图片"></p>
<p>这意味着，神经网络具有一种普遍性，不论需要计算的是什么样的函数，我们都确定存在一个网络架构能够计算这个问题。<br>关键点在于，我们并没有真正获取函数对应的数学公式，我们也不知道真实的解是多少，但是我们可以获得一个近似值，并且这个近似值对应的误差是可控的。同时，我们的网络架构具备普适性，能够面对任意的函数。如果将函数推广到需要解决的问题：</p>
<ul>
<li>f(x) 代表将中文翻译为对应的英文</li>
<li>f(x) 代表将混合的多源语音进行分离</li>
<li>f(x) 代表将人脸图片映射为代表的人</li>
</ul>
<p>神经网络的这种普适性保证了我们能够解决『所有问题』。</p>
<p>可能存在的一个问题是，上述的普适性，面对的函数是连续的，如果函数是非连续的，类似阶跃函数这样的情景下，神经网络就不适应。</p>
<h3 id="如何解决"><a href="#如何解决" class="headerlink" title="如何解决"></a>如何解决</h3><p>前两节只是描述了神经网络能够对连续函数的输出值进行一个估计，并将误差控制在一个阈值内。那网络是如何解决这个问题？</p>
<p>我们先<strong><em>简化</em></strong>问题规模，解决的f(x)只有一维输入和一维输出，首先关注最上面的神经元。<br><img src="http://mmbiz.qpic.cn/mmbiz/58FUuNaBUjrcxwHE3MQxbaWKZgIr4VicavdQPVbFC5gAk6ClA1GkQJyEe9URv51geiaibQ5dDTgR7KgiaxsPw6afSw/0?wx_fmt=gif&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1" alt="图片"></p>
<ul>
<li>增加w的值，对应的输出将逐渐有sigmoid函数变化为阶跃函数</li>
<li>改变b的值，将平移输出结果</li>
</ul>
<blockquote>
<p>这里之所以要用阶跃函数取代sigmoid函数来分析问题，主要是一系列阶跃函数的累加值分析更加直观简单，相反，一连串s型的曲线叠加在一起是什么就更难分析</p>
</blockquote>
<p>关键点就在于阶跃函数发生阶跃的位置，这个位置可以由b控制，同时令s=-b/w，可以得到下述表现形式<br><img src="http://bos.nj.bpc.baidu.com/v1/agroup/5e47548e2052dc9de1fb5feda55a1b294eb10ff5" alt="图片"></p>
<p>我们控制w1的值，就可以控制阶跃的高度，两个阶跃叠加的结果</p>
<p><img src="http://bos.nj.bpc.baidu.com/v1/agroup/ceb016eca512dfa576f384b6988cf06cbdc856fa" alt="图片"></p>
<p>如果w1和w2的值相反，正负叠加的结果为一个单峰的阶跃值。<br><img src="http://bos.nj.bpc.baidu.com/v1/agroup/8473e532e2d1f8eb7a08c6c912d460a510ab92a7" alt="图片"></p>
<p>上述叠加发生在存在两个配对的神经元间，如果我们增加神经元的数量，则可以通过叠加获得更加丰富的结果。<br><img src="http://bos.nj.bpc.baidu.com/v1/agroup/6f34e2901e35670e82061623df6c82f679bc1b06" alt="图片"><br>现在回到最早的问题，我们要解决一个特殊的函数<br><img src="http://bos.nj.bpc.baidu.com/v1/agroup/2c1de2e0b8af6c515a3c131d77f9e71cb5108d9b" alt="图片"><br>这个函数实际上对应的函数是<br><img src="http://bos.nj.bpc.baidu.com/v1/agroup/1e5b091b6e9f6df72d8da0ed4fedef37f6501a4a" alt="图片"><br>虽然我们并不知道，但通过多个阶跃值进行模拟，就可以获得一个粗略的模拟值和一个与真实值自建的差异<br><img src="http://bos.nj.bpc.baidu.com/v1/agroup/f938a71c2fb21de7e377496f57fe2de51de7c631" alt="图片"><br>通过控制各个s与w的值，就能够保证在可以接受的误差阈值内获取一个模型，这个模型能够尽量接近真实结果，这就是神经网络能够训练所有问题的基础。</p>
<h2 id="后序"><a href="#后序" class="headerlink" title="后序"></a>后序</h2><ul>
<li><p>上述内容是在学习《神经网络与深度学习》之后的一些感悟，总结的有点杂乱，大概描述了看这本书过程中一些重要的理解。然后我决定以后看一本书，就写一篇读书笔记，花了一个月的时间，总不能就这样什么都没留下是吧。</p>
</li>
<li><p>看这本书，源自于某两人在某次开会是争论深度学习与机器学习是不是完全不同的两种技术。</p>
</li>
<li><p>以前还在糯米的时候，听某晓大神说过，神经网络与深度学习就是把一堆输入提供给网络，我不知道它是怎么训练的，各个中间值是什么意思，代表什么。这是深度学习和机器学习特别是监督学习最本质的区别。</p>
</li>
<li><p>看完nndl，要说能有什么马上能转化为产出的地方，其实并没有。但是，很多年前，大保健就说过，你花在读书上的时间永远不会浪费，哪怕这个东西你现在用不上，哪怕你过段时间都快忘了，总有一天，当你突然需要用时，泉思如涌，一切都是自然而然。</p>
</li>
<li><p>看这本书的时候，我一直想起很多年前泡图书馆的日子，悠闲自在，有大把大把的时间可以去做自己想做的事。但是，那时候傻啊，屁都不懂就特么闷头学.NET，还看不懂英文书，白特么瞎了那么多时间。刚真，我适合搞学术。</p>
</li>
<li><p>然后，没然后了，上了年纪不比当年，反应有点慢。就这样吧，下本看的书是《Deep Learning》。</p>
</li>
<li><p>最后的最后，曾经有人看了文章，迫于我的气场，给我打赏了一分钱，没错，就特么一分钱，我祝他短1cm，张小哥，说的就是你。</p>
</li>
</ul>

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        
  <div style="padding: 10px 0; margin: 20px auto; width: 90%; text-align: center;">
    <div>坚持原创技术分享，您的支持将鼓励我继续创作！</div>
    <button id="rewardButton" disable="enable" onclick="var qr = document.getElementById('QR'); if (qr.style.display === 'none') {qr.style.display='block';} else {qr.style.display='none'}">
      <span>赏</span>
    </button>
    <div id="QR" style="display: none;">
      
        <div id="wechat" style="display: inline-block">
          <img id="wechat_qr" src="/uploads/wechat-reward-image" alt="k神 WeChat Pay">
          <p>微信打赏</p>
        </div>
      
      
        <div id="alipay" style="display: inline-block">
          <img id="alipay_qr" src="/uploads/alipay-reward-image" alt="k神 Alipay">
          <p>支付宝打赏</p>
        </div>
      
    </div>
  </div>


      
    </div>


    <footer class="post-footer">
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2017/05/07/《神经网络与深度学习》学习笔记/" rel="next" title="《神经网络与深度学习》学习笔记">
                <i class="fa fa-chevron-left"></i> 《神经网络与深度学习》学习笔记
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          

  <p>热评文章</p>
  <div class="ds-top-threads" data-range="weekly" data-num-items="4"></div>


          
  <div class="comments" id="comments">
    
      <div class="ds-thread" data-thread-key="2017/05/07/bak/" data-title="《神经网络与深度学习》学习笔记" data-url="http://yoursite.com/2017/05/07/bak/">
      </div>
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image" src="/uploads/avatar.png" alt="k神">
          <p class="site-author-name" itemprop="name">k神</p>
          <p class="site-description motion-element" itemprop="description">做一名纯粹中庸的小白</p>
        </div>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">4</span>
              <span class="site-state-item-name">日志</span>
            </a>
          </div>

          

          
            <div class="site-state-item site-state-tags">
              <a href="/tags">
                <span class="site-state-item-count">1</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        
          <div class="feed-link motion-element">
            <a href="/atom.xml" rel="alternate">
              <i class="fa fa-rss"></i>
              RSS
            </a>
          </div>
        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/proxukun" target="_blank" title="GitHub" rel="external nofollow noopener noreferrer">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                  GitHub
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://www.douban.com/people/85375509/" target="_blank" title="豆瓣" rel="external nofollow noopener noreferrer">
                  
                    <i class="fa fa-fw fa-globe"></i>
                  
                  豆瓣
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://www.zhihu.com/people/proxukun/activities" target="_blank" title="知乎" rel="external nofollow noopener noreferrer">
                  
                    <i class="fa fa-fw fa-globe"></i>
                  
                  知乎
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="http://t.qq.com/proxukun" target="_blank" title="微博" rel="external nofollow noopener noreferrer">
                  
                    <i class="fa fa-fw fa-globe"></i>
                  
                  微博
                </a>
              </span>
            
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#目录"><span class="nav-number">1.</span> <span class="nav-text">目录</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#神经网络进行学习的基本前提"><span class="nav-number">2.</span> <span class="nav-text">神经网络进行学习的基本前提</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#神经网络两个假设与四个等式"><span class="nav-number">3.</span> <span class="nav-text">神经网络两个假设与四个等式</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#代价函数的两个假设"><span class="nav-number">3.1.</span> <span class="nav-text">代价函数的两个假设</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#假设1"><span class="nav-number">3.1.1.</span> <span class="nav-text">假设1</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#假设2"><span class="nav-number">3.1.2.</span> <span class="nav-text">假设2</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#BP背后的四个基本等式"><span class="nav-number">3.2.</span> <span class="nav-text">BP背后的四个基本等式</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#BP1"><span class="nav-number">3.2.1.</span> <span class="nav-text">BP1</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#BP2"><span class="nav-number">3.2.2.</span> <span class="nav-text">BP2</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#BP3"><span class="nav-number">3.2.3.</span> <span class="nav-text">BP3</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#BP4"><span class="nav-number">3.2.4.</span> <span class="nav-text">BP4</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#实现"><span class="nav-number">4.</span> <span class="nav-text">实现</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#超参数的选择"><span class="nav-number">5.</span> <span class="nav-text">超参数的选择</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#mini-batch-size"><span class="nav-number">5.1.</span> <span class="nav-text">mini_batch_size</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#epoll"><span class="nav-number">5.2.</span> <span class="nav-text">epoll</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#lamda"><span class="nav-number">5.3.</span> <span class="nav-text">lamda</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#η"><span class="nav-number">5.4.</span> <span class="nav-text">η</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#存在的问题与优化点"><span class="nav-number">6.</span> <span class="nav-text">存在的问题与优化点</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#学习速率的优化"><span class="nav-number">6.1.</span> <span class="nav-text">学习速率的优化</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#学习速率慢的真正原因"><span class="nav-number">6.1.1.</span> <span class="nav-text">学习速率慢的真正原因</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#交叉熵解决方案"><span class="nav-number">6.1.2.</span> <span class="nav-text">交叉熵解决方案</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#过拟合的抑制"><span class="nav-number">6.2.</span> <span class="nav-text">过拟合的抑制</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#为什么神经网络能够『解决』『所有』问题"><span class="nav-number">7.</span> <span class="nav-text">为什么神经网络能够『解决』『所有』问题</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#『解决』的含义"><span class="nav-number">7.1.</span> <span class="nav-text">『解决』的含义</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#『所有』的含义"><span class="nav-number">7.2.</span> <span class="nav-text">『所有』的含义</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#如何解决"><span class="nav-number">7.3.</span> <span class="nav-text">如何解决</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#后序"><span class="nav-number">8.</span> <span class="nav-text">后序</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js">
</script>
<div class="copyright">
  
  &copy;  2015 - 
  <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">k神</span>

</div>


        

        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  



  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.0"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.0"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script>



  

  
    
  

  <script type="text/javascript">
    var duoshuoQuery = {short_name:"proxukun"};
    (function() {
      var ds = document.createElement('script');
      ds.type = 'text/javascript';ds.async = true;
      ds.id = 'duoshuo-script';
      ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
      ds.charset = 'UTF-8';
      (document.getElementsByTagName('head')[0]
      || document.getElementsByTagName('body')[0]).appendChild(ds);
    })();
  </script>

  
    
    
    <script src="/lib/ua-parser-js/dist/ua-parser.min.js?v=0.7.9"></script>
    <script src="/js/src/hook-duoshuo.js"></script>
  








  
  
  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length == 0) {
      search_path = "search.xml";
    }
    var path = "/" + search_path;
    // monitor main search box;

    function proceedsearch() {
      $("body").append('<div class="popoverlay">').css('overflow', 'hidden');
      $('.popup').toggle();
    }
    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';
      $.ajax({
        url: path,
        dataType: "xml",
        async: true,
        success: function( xmlResponse ) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = $( "entry", xmlResponse ).map(function() {
            return {
              title: $( "title", this ).text(),
              content: $("content",this).text(),
              url: $( "url" , this).text()
            };
          }).get();
          var $input = document.getElementById(search_id);
          var $resultContent = document.getElementById(content_id);
          $input.addEventListener('input', function(){
            var matchcounts = 0;
            var str='<ul class=\"search-result-list\">';
            var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
            $resultContent.innerHTML = "";
            if (this.value.trim().length > 1) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var content_index = [];
                var data_title = data.title.trim().toLowerCase();
                var data_content = data.content.trim().replace(/<[^>]+>/g,"").toLowerCase();
                var data_url = decodeURIComponent(data.url);
                var index_title = -1;
                var index_content = -1;
                var first_occur = -1;
                // only match artiles with not empty titles and contents
                if(data_title != '') {
                  keywords.forEach(function(keyword, i) {
                    index_title = data_title.indexOf(keyword);
                    index_content = data_content.indexOf(keyword);
                    if( index_title >= 0 || index_content >= 0 ){
                      isMatch = true;
                      if (i == 0) {
                        first_occur = index_content;
                      }
                    }

                  });
                }
                // show search results
                if (isMatch) {
                  matchcounts += 1;
                  str += "<li><a href='"+ data_url +"' class='search-result-title'>"+ data_title +"</a>";
                  var content = data.content.trim().replace(/<[^>]+>/g,"");
                  if (first_occur >= 0) {
                    // cut out 100 characters
                    var start = first_occur - 20;
                    var end = first_occur + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if(start == 0){
                      end = 50;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    var match_content = content.substring(start, end);
                    // highlight all keywords
                    keywords.forEach(function(keyword){
                      var regS = new RegExp(keyword, "gi");
                      match_content = match_content.replace(regS, "<b class=\"search-keyword\">"+keyword+"</b>");
                    });

                    str += "<p class=\"search-result\">" + match_content +"...</p>"
                  }
                  str += "</li>";
                }
              })};
            str += "</ul>";
            if (matchcounts == 0) { str = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>' }
            if (keywords == "") { str = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>' }
            $resultContent.innerHTML = str;
          });
          proceedsearch();
        }
      });}

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched == false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(function(e){
      $('.popup').hide();
      $(".popoverlay").remove();
      $('body').css('overflow', '');
    });
    $('.popup').click(function(e){
      e.stopPropagation();
    });
  </script>


  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.1.js"></script>
  <script>AV.initialize("9gx431jY99kp4LtAGkaorKmO-gzGzoHsz", "KGlUxue9je1nwznXa4xId6pz");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
          for(var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = document.getElementById(url);
            var countSpan = $(element).find(COUNT_CONTAINER_REF);
            if( countSpan.text() == '') {
              countSpan.text(0);
            }
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            /* Set ACL */
            var acl = new AV.ACL();
            acl.setPublicReadAccess(true);
            acl.setPublicWriteAccess(true);
            newcounter.setACL(acl);
            /* End Set ACL */
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>



  

  


</body>
</html>
